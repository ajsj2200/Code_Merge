{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajsj2\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from google.api_core import retry\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyAJK_qmrMzAkK0mENVwjKHZD-WsJ32v35w'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = pathlib.Path('gemini_api_key.txt').read_text().strip()\n",
    "genai.configure(api_key=api_key)\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_story = False\n",
    "\n",
    "if new_story:\n",
    "  model = genai.GenerativeModel(model_name='models/gemini-1.5-flash-lastest')\n",
    "\n",
    "  response = model.generate_content(\"\"\"\n",
    "      Write a long story about a girl with magic backpack, her family, and at\n",
    "      least one other charater. Make sure everyone has names. Don't forget to\n",
    "      describe the contents of the backpack, and where everyone and everything\n",
    "      starts and ends up.\"\"\", request_options={'retry': retry.Retry()})\n",
    "  story = response.text\n",
    "  print(response.candidates[0].citation_metadata)\n",
    "else:\n",
    "  story = \"\"\"In the quaint town of Willow Creek, nestled amidst rolling hills and whispering willows, resided a young girl named Anya. As she stepped out of the creaky wooden door of her modest cottage, her heart skipped a beat with excitement and anticipation. Today was her first day of school, and she couldn't wait to show off her prized possession - a magical backpack.\\n\\nHanded down to her from her grandmother, the backpack was no ordinary satchel. Its soft, emerald-green fabric shimmered with an ethereal glow, and its leather straps held secrets that only Anya knew. Within its capacious interior lay an enchanted world, filled with wonders that would ignite her imagination and change her life forever.\\n\\nAnya's parents, kind-hearted Elise and wise-bearded Edward, bid her farewell with warm embraces. \"Remember, my dear,\" whispered her mother, \"use your magic wisely and for good.\" Her father added, \"Always seek knowledge, and let the backpack be your trusted companion.\"\\n\\nWith a skip in her step, Anya set off towards the town's only schoolhouse. On her way, she passed her best friend, Samuel, a curious and adventurous boy with a mischievous grin. \"Hey, Anya,\" he called out. \"Can I see your backpack?\"\\n\\nAnya hesitated for a moment before unzipping the flap and revealing its contents. Samuel's eyes widened in amazement as he peered inside. There, nestled amidst pencils and notebooks, were a shimmering sword, a book of ancient spells, a tiny compass that always pointed north, and a magical key that could open any lock.\\n\\nTogether, they marveled at the backpack's wonders, promising to keep its secrets safe. As they approached the schoolhouse, Anya noticed a group of older children huddled together, their faces etched with fear. Curiosity getting the better of her, she cautiously approached.\\n\\n\"What's wrong?\" she asked.\\n\\nA tall, lanky boy stepped forward. \"There's a monster in the forest,\" he stammered. \"It's been terrorizing the town, attacking animals and even people.\"\\n\\nAnya's heart sank. The town of Willow Creek was small and peaceful, and the thought of a monster brought a shiver down her spine. She knew she had to do something to protect her family and friends.\\n\\nWithout a moment's hesitation, Anya opened her backpack and retrieved the shimmering sword. With a determined gleam in her eye, she turned to her terrified peers. \"Don't worry,\" she said, her voice steady. \"I'll take care of it.\"\\n\\nWith Samuel close behind her, Anya ventured into the shadowy depths of the forest. The trees seemed to whisper secrets as she passed, and the undergrowth rustled with unseen creatures. As they walked deeper into the forest, the air grew heavy and the ground beneath their feet trembled.\\n\\nSuddenly, they came to a clearing, and there before their eyes was the monster - a massive beast with sharp teeth, glowing red eyes, and claws that could crush a human with ease. The creature roared, a thunderous sound that shook the forest to its core.\\n\\nFear surged through Anya, but she refused to let it consume her. She drew the sword from its sheath and charged towards the monster. The blade shimmered in the sunlight, and as it struck the beast's hide, a blinding light erupted, enveloping everything in its radiance.\\n\\nWhen the light faded, the monster was gone, and in its place was a pile of shattered crystals. Anya had defeated the creature with the magic of her backpack, proving that even the smallest of objects could hold the greatest of powers.\\n\\nAs she and Samuel returned to the town, they were greeted as heroes. The people of Willow Creek rejoiced, and the legend of Anya, the girl with the magic backpack, was passed down through generations. And so, Anya continued her adventures, using the backpack's wonders to make the world a better place, one magical step at a time.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash-latest')\n",
    "\n",
    "response = model.generate_content(\n",
    "  textwrap.dedent(\"\"\"\\\n",
    "    Please return JSON describing the the people, places, things and relationships from this story using the following schema:\n",
    "\n",
    "    {\"people\": list[PERSON], \"places\":list[PLACE], \"things\":list[THING], \"relationships\": list[RELATIONSHIP]}\n",
    "\n",
    "    PERSON = {\"name\": str, \"description\": str, \"start_place_name\": str, \"end_place_name\": str}\n",
    "    PLACE = {\"name\": str, \"description\": str}\n",
    "    THING = {\"name\": str, \"description\": str, \"start_place_name\": str, \"end_place_name\": str}\n",
    "    RELATIONSHIP = {\"person_1_name\": str, \"person_2_name\": str, \"relationship\": str}\n",
    "\n",
    "    All fields are required.\n",
    "\n",
    "    Important: Only return a single piece of valid JSON text.\n",
    "\n",
    "    Here is the story:\n",
    "\n",
    "    \"\"\") + story,\n",
    "  generation_config={'response_mime_type':'application/json'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에너지 효율을 극대화하기 위해 운전 데이터 기반 진단 기술, 다중 에너지 네트워크 설계 및 현장 설치 기술을 통합하여 유틸리티 설비 운전 최적화 시스템을 구축한다.\n",
      "운전 데이터 분석을 기반으로 유틸리티 설비의 효율적인 운전을 위한 진단 및 설계 기술을 개발하고, 다중 에너지 네트워크를 구축하여 에너지 효율성을 높이는 것을 목표로 한다.\n",
      "운전 데이터 분석, 다중 에너지 네트워크 설계 및 현장 설치 기술을 연동하여 유틸리티 설비의 에너지 효율을 최적화하고 운전 효율성을 향상시키는 시스템을 구축하는 것을 목표로 한다.\n",
      "운전 데이터 기반 진단 기술, 다중 에너지 네트워크 설계 및 현장 설치 기술을 활용하여 유틸리티 설비의 에너지 효율성을 극대화하고 지속 가능한 운영을 위한 시스템을 구축하는 것을 목표로 한다.\n",
      "운전 데이터 분석을 통해 유틸리티 설비의 에너지 효율성을 개선하고, 다중 에너지 네트워크 설계 및 현장 설치를 통해 에너지 효율성을 극대화하는 시스템 구축을 목표로 한다.\n",
      "-----------------------------------\n",
      "에너지 진단 및 다중에너지 네트워크 구축을 통해 에너지 효율을 극대화하는 시스템을 구축하는 것을 목표로 합니다.\n",
      "운전 데이터 기반 에너지 진단 및 다중에너지 네트워크 설계 및 현장 설치를 통해 에너지 효율을 향상시키는 시스템을 구축하는 것을 목표로 합니다.\n",
      "운전 데이터 기반 진단, 다중에너지 네트워크 설계, 현장 설치를 통해 에너지 효율 향상 및 최적화된 시스템 구축을 목표로 합니다.\n",
      "운전 데이터를 활용하여 에너지 진단 및 다중에너지 네트워크를 설계하고 현장에 설치하여 에너지 효율을 높이는 것을 목표로 합니다.\n",
      "에너지 효율 향상을 위해 운전 데이터 기반 에너지 진단, 다중에너지 네트워크 설계, 현장 설치를 수행하는 것을 목표로 합니다.\n",
      "-----------------------------------\n",
      "에너지 진단 및 다중에너지 네트워크 설계, 현장 설치를 통해 운전 데이터 기반 최적화된 유틸리티 운전 환경을 구축하는 것을 목표로 한다.\n",
      "운전 데이터를 활용하여 유틸리티 설비의 효율성을 향상시키고, 다중에너지 네트워크를 통해 에너지 관리를 최적화하는 것을 목표로 한다.\n",
      "에너지 효율 향상 및 운영 안정성 확보를 위해 운전 데이터 기반 진단, 설계, 구축 기술을 적용하는 것을 목표로 한다.\n",
      "다중에너지 네트워크를 기반으로 운전 데이터를 분석하고, 최적화된 에너지 시스템을 설계 및 구축하는 것을 목표로 한다.\n",
      "운전 데이터 분석을 통해 유틸리티 시스템을 진단하고, 다중에너지 네트워크를 설계 및 구축하여 에너지 효율을 극대화하는 것을 목표로 한다.\n",
      "-----------------------------------\n",
      "에너지 효율을 극대화하고 운영 비용을 절감하기 위해 운전 데이터 기반 에너지 진단 기술과 다중에너지 네트워크 설계 및 현장 설치 기술을 통합하여 유틸리티 설비의 최적 운전을 위한 시스템을 구축합니다.\n",
      "운전 데이터 분석을 통해 유틸리티 설비의 최적 운전 방안을 도출하고 다중에너지 네트워크를 설계하여 현장에 설치함으로써 에너지 효율성을 향상시키는 시스템 구축을 목표로 합니다.\n",
      "운전 데이터 기반 진단, 다중에너지 네트워크 설계 및 현장 설치 기술을 활용하여 에너지 효율을 높이고 운영 비용을 절감하는 통합 시스템을 구축합니다.\n",
      "에너지 진단 기술, 다중에너지 네트워크 설계 및 현장 설치 기술을 연계하여 유틸리티 설비의 에너지 효율을 최적화하고 운영 비용을 절감하는 시스템을 구축하는 것을 목표로 합니다.\n",
      "운전 데이터 분석, 다중에너지 네트워크 설계, 현장 설치 기술을 통해 유틸리티 설비의 에너지 효율을 향상시키고 운영 비용을 절감하는 시스템을 구축하는 것을 목표로 합니다.\n",
      "-----------------------------------\n",
      "에너지 진단 기술을 활용하여 운전 데이터 기반 최적화를 통해 유틸리티 설비의 효율성을 향상시키고, 다중에너지 네트워크 설계 및 현장 설치 기술을 통해 에너지 효율성을 극대화하는 것을 목표로 한다.\n",
      "본 문서는 에너지 진단, 다중에너지 네트워크 설계 및 현장 설치 기술을 통해 유틸리티 설비의 효율적인 운영과 에너지 절감을 목표로 한다.\n",
      "본 문서는 운전 데이터 분석을 통한 에너지 진단, 다중에너지 네트워크 설계 및 현장 설치 기술을 통해 에너지 효율성을 향상시키는 것을 목표로 한다.\n",
      "에너지 진단 기술, 다중에너지 네트워크 설계 및 현장 설치 기술을 통해 유틸리티 시스템의 운영 효율성을 높이고 에너지 절약을 목표로 한다.\n",
      "운전 데이터 기반 에너지 진단, 다중에너지 네트워크 설계 및 현장 설치를 통해 유틸리티 시스템의 에너지 효율성을 향상시키고, 최적화된 운영 환경을 구축하는 것을 목표로 한다.\n",
      "-----------------------------------\n",
      "에너지 효율성을 극대화하고 운영비용을 절감하기 위해 운전 데이터 기반 에너지 진단 기술, 다중에너지 네트워크 설계 및 현장 설치 기술을 통합하여 효율적인 시스템을 구축하는 것이 목표입니다.\n",
      "운전 데이터를 활용하여 에너지 시스템을 진단하고, 다중에너지 네트워크를 설계 및 구축하여 에너지 효율성을 높이고 비용을 절감하는 것이 목표입니다.\n",
      "에너지 진단, 설계, 현장 설치를 통해 효율적인 다중에너지 네트워크를 구축하여 에너지 소비를 최적화하고 운영 비용을 절감하는 것이 목표입니다.\n",
      "운전 데이터 분석, 다중에너지 네트워크 설계 및 현장 설치를 통해 에너지 시스템의 효율성을 극대화하고 운영 비용을 최소화하는 것을 목표로 합니다.\n",
      "다중에너지 네트워크를 구축하여 에너지 효율성을 향상시키고 운영 비용을 절감하는 데 목표를 두고 있습니다. 이를 위해 운전 데이터를 기반으로 에너지 진단 및 설계를 수행하고 현장 설치를 통해 시스템을 구축합니다.\n",
      "-----------------------------------\n",
      "본 문서는 운전 데이터 기반 에너지 진단 기술, 다중에너지 네트워크 설계 기술, 현장 설치 기술을 통해 에너지 효율을 극대화하고 최적의 운전 환경을 구축하는 것을 목표로 합니다.\n",
      "본 문서는 운전 데이터 기반 에너지 진단 기술을 활용하여 운전 최적화를 위한 알고리즘을 개발하고, 다중에너지 네트워크 설계 및 현장 설치를 통해 에너지 효율성을 극대화하는 것을 목표로 합니다.\n",
      "본 문서는 운전 데이터를 활용하여 에너지 진단 및 최적화 기술을 개발하고, 다중에너지 네트워크 설계 및 현장 설치를 통해 효율적인 에너지 시스템 구축을 목표로 합니다.\n",
      "본 문서는 운전 데이터 기반 에너지 진단 기술, 다중에너지 네트워크 설계 및 현장 설치 기술을 통합하여 에너지 시스템의 효율성을 높이고 최적화된 운영 환경을 구축하는 것을 목표로 합니다.\n",
      "본 문서는 운전 데이터 분석을 통해 에너지 효율을 향상시키고, 다중에너지 네트워크 설계 및 현장 설치 기술을 통해 효율적인 에너지 시스템 구축을 목표로 합니다.\n",
      "-----------------------------------\n",
      "에너지 효율을 극대화하기 위해 운전 데이터를 기반으로 유틸리티 설비의 최적 운전을 위한 진단, 설계, 구축 기술을 개발하는 것을 목표로 한다.\n",
      "다중 에너지 네트워크 시스템의 운전 효율을 향상시키기 위해 운전 데이터 기반 진단, 설계, 현장 설치 기술을 개발하여 에너지 효율을 극대화하는 것을 목표로 한다.\n",
      "본 문서는 운전 데이터를 기반으로 에너지 진단, 설계, 현장 설치 기술을 개발하여 다중 에너지 네트워크 시스템의 효율성을 향상시키는 것을 목표로 한다.\n",
      "본 문서는 다중 에너지 네트워크 시스템의 에너지 효율을 향상시키기 위해 운전 데이터 기반 진단, 설계, 구축 기술을 개발하는 것을 목표로 한다.\n",
      "운전 데이터 기반 에너지 진단, 설계, 현장 설치 기술을 개발하여 다중 에너지 네트워크 시스템의 에너지 효율을 극대화하는 것을 목표로 한다.\n",
      "-----------------------------------\n",
      "에너지 진단 및 다중에너지 네트워크 구축을 통해 에너지 효율을 극대화하고, 운전 최적화를 위한 시스템을 구축하는 것이 목표입니다.\n",
      "운전 데이터 기반 에너지 진단 기술과 다중에너지 네트워크 설계 및 현장 설치 기술을 통해 에너지 효율성을 높이고 운전 최적화를 목표로 합니다.\n",
      "운전 데이터 분석을 통해 에너지 진단, 다중에너지 네트워크 설계 및 현장 설치를 수행하여 에너지 효율 개선 및 최적 운영을 목표로 합니다.\n",
      "에너지 진단 기술, 다중에너지 네트워크 설계 및 현장 설치 기술을 통합하여 에너지 효율을 향상시키고 운전 효율을 극대화하는 것이 목표입니다.\n",
      "에너지 진단, 다중에너지 네트워크 구축을 통해 에너지 소비를 최적화하고 효율적인 운영 환경을 구축하는 것을 목표로 합니다.\n",
      "-----------------------------------\n",
      "에너지 효율성을 극대화하기 위해 운전 데이터 기반 진단 및 설계 기술을 통해 유틸리티 운전을 최적화하고, 다중에너지 네트워크 설계 및 현장 설치 기술을 통해 효율적인 에너지 관리 시스템을 구축하는 것을 목표로 한다.\n",
      "운전 데이터 분석을 기반으로 에너지 진단 및 설계 기술을 활용하여 유틸리티 운전을 최적화하고, 다중에너지 네트워크를 구축하여 에너지 효율성을 향상시키는 것을 목표로 한다.\n",
      "운전 데이터 기반 에너지 진단 기술, 다중에너지 네트워크 설계 및 현장 설치 기술을 통합하여 에너지 효율성을 극대화하고 지속 가능한 에너지 관리 시스템을 구축하는 것을 목표로 한다.\n",
      "운전 데이터를 활용하여 유틸리티 운전을 최적화하고, 다중에너지 네트워크를 구축하여 에너지 효율성을 높이는 것을 목표로 하는 에너지 관리 시스템 구축을 위한 기술 개발을 목표로 한다.\n",
      "에너지 효율성 향상을 목표로 운전 데이터 분석 기반 진단 및 설계 기술을 활용하여 유틸리티 운전을 최적화하고, 다중에너지 네트워크 설계 및 구축을 통해 효율적인 에너지 관리 시스템을 구축하는 것을 목표로 한다.\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "from IPython.display import display, Markdown\n",
    "from google.api_core import retry\n",
    "import concurrent.futures\n",
    "\n",
    "def to_markdown(text):\n",
    "    text = text.replace('•', '  *')\n",
    "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "def load_api_key(filepath):\n",
    "    return pathlib.Path(filepath).read_text().strip()\n",
    "\n",
    "def configure_genai(api_key):\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "def get_model(model_name):\n",
    "    generation_config = {\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 64,\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"response_mime_type\": \"text/plain\",\n",
    "        }\n",
    "    \n",
    "    return genai.GenerativeModel(model_name=model_name, generation_config=generation_config)\n",
    "\n",
    "def get_recommendations(model, context, num_candidates, num_blocks):\n",
    "    prompt = f\"\"\"\n",
    "    Please return JSON describing the recommended next blocks for the given context:\n",
    "    {context}\n",
    "\n",
    "    Return {num_candidates} candidates each with {num_blocks} blocks.\n",
    "    각 후보끼리는 서로 다른 문장을 추천해야하며 순서가 없어야 합니다.\n",
    "    Example output format:\n",
    "    [\n",
    "        {{\n",
    "            \"candidate\": 1,\n",
    "            \"blocks\": [\n",
    "                \"Block 1 text.\",\n",
    "                \"Block 2 text.\"\n",
    "            ]\n",
    "        }},\n",
    "        {{\n",
    "            \"candidate\": 2,\n",
    "            \"blocks\": [\n",
    "                \"Block 1 text.\",\n",
    "                \"Block 2 text.\"\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt, request_options={'retry': retry.Retry()})\n",
    "    return response\n",
    "\n",
    "def process_response(response):\n",
    "    try:\n",
    "        recommendations = response.candidates[0].content.parts[0].text\n",
    "        return recommendations\n",
    "    except KeyError:\n",
    "        return []\n",
    "\n",
    "def display_recommendations(recommendations):\n",
    "    for rec in recommendations:\n",
    "        display(to_markdown(rec))\n",
    "\n",
    "# API 키 로드 및 설정\n",
    "api_key_filepath = 'gemini_api_key.txt'\n",
    "api_key = load_api_key(api_key_filepath)\n",
    "configure_genai(api_key)\n",
    "\n",
    "# 문맥 설정 및 추천 블록 수 설정\n",
    "context = \"\"\"\n",
    "(운전 데이터 기반) 에너지 진단 기술 - **진단, 설계 담당**\n",
    "\n",
    "1. 운전점(운전변수) 정의 및 체계 구성\n",
    "    1. 유틸리티 운전점\n",
    "        - 유틸리티설비의 최적 운전을 위한 제어변수 구성 - 체계 수립 후 라이브러리화\n",
    "2. 운전합리화 방안 도출 - 운전가이드 제시\n",
    "    1. 단독 운전시 운전합리화 방안\n",
    "        1. 단독 제어 알고리즘 구성\n",
    "\n",
    "(다중에너지 네트워크) 설계 기술  - **진단, 설계 담당**\n",
    "\n",
    "1. 인프라 설계\n",
    "    1. 데이터 수집 인프라\n",
    "        1. 관제점 구성. 인터페이스 방안. \n",
    "    2. 통신 네트워크 인프라\n",
    "        1. 네트워크 구성 및 상호 연결 프로토콜 설계\n",
    "    3. (현장)제어 인프라\n",
    "        1. 현장 설비 제어 인터페이스 제공(API)\n",
    "2. 시스템 설계\n",
    "    1. (현장)제어 시스템\n",
    "        1. 설비 제어 알고리즘 설계 및 제어 시스템 구축\n",
    "\n",
    "(다중에너지 네트워크) 현장설치 기술 - **구축(설치) 담당**\n",
    "\n",
    "1. (운전)제어시스템 설치 - 현장에서 운전원이 직접 제어할 수 있는 환경 구성\n",
    "    1. 제어 인프라 설치\n",
    "        1. 계장장치(PLC/DCS/DDC), 통신선 구성\n",
    "    2. 제어시스템\n",
    "        1. Sequence에 따른 제어. 장애 대응시 단독 운전 전환\n",
    "    3. 최적화시스템\n",
    "        1. 유틸리티 운전최적화 알고리즘 적용\n",
    "        \n",
    "[위를 1문장으로 요약해줘. 목적을 적어야해.]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 추천받은 문장에 대한 후보 수\n",
    "num_candidates = 5\n",
    "\n",
    "# 추천받을 문장 수\n",
    "num_blocks = 1\n",
    "\n",
    "# 병렬 API 개수\n",
    "num_parallel_apis = 10\n",
    "# 모델 설정\n",
    "model_name = 'models/gemini-1.5-flash-latest'\n",
    "model = get_model(model_name)\n",
    "\n",
    "# 병렬 API 호출 수행\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    \n",
    "    futures = []\n",
    "    for _ in range(num_parallel_apis):\n",
    "        futures.append(executor.submit(get_recommendations, model, context, num_candidates, num_blocks))\n",
    "\n",
    "    responses = [future.result() for future in futures]\n",
    "\n",
    "# 응답 합치기\n",
    "combined_recommendations = []\n",
    "for response in responses:\n",
    "    recommendations = process_response(response)\n",
    "    combined_recommendations.append(recommendations)\n",
    "\n",
    "# # 추천 블록 출력\n",
    "# display_recommendations(combined_recommendations)\n",
    "\n",
    "import re\n",
    "\n",
    "json_string = combined_recommendations\n",
    "\n",
    "blocks_pattern = re.compile(r'\"blocks\": \\[\\s*\"(.*?)\"\\s*\\]', re.DOTALL)\n",
    "blocks = [blocks_pattern.findall(x) for x in json_string]\n",
    "\n",
    "for b in blocks:\n",
    "    [print(x) for x in b]\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에너지 진단 및 다중 에너지 네트워크 기술을 활용하여 운전 최적화, 인프라 구축 및 현장 제어 시스템 설계를 통해 에너지 효율성을 극대화하는 것을 목표로 한다.\n",
      "-----------------------------------\n",
      "에너지 진단 기술은 운전 데이터 분석을 통해 최적 운전 가이드를 제공하고, 다중 에너지 네트워크 기술은 인프라 및 시스템 설계를 통해 현장 제어 및 최적화 시스템 구축을 수행한다.\n",
      "-----------------------------------\n",
      "에너지 진단 기술은 운전 데이터를 활용하여 최적 운전 가이드를 제공하고, 다중 에너지 네트워크 기술은 인프라 및 시스템 설계를 통해 현장 제어 및 최적화 시스템을 구축하여 에너지 효율을 향상시키는 것을 목표로 한다.\n",
      "-----------------------------------\n",
      "본 기술은 운전 데이터를 활용하여 에너지 효율을 높이는 최적 운전 가이드를 제공하고, 다중 에너지 네트워크를 설계 및 구축하여 현장 제어 및 최적화 시스템을 구축하는 것을 목표로 한다.\n",
      "-----------------------------------\n",
      "운전 데이터 기반 에너지 진단 기술은 최적 운전 가이드를 제공하고, 다중 에너지 네트워크 기술은 인프라, 시스템 설계 및 현장 제어를 통해 에너지 효율을 향상시키는 것을 목표로 한다.\n",
      "-----------------------------------\n",
      "운전 데이터 기반 에너지 진단 기술은 최적 운전 가이드를 제공하고, 다중 에너지 네트워크 기술은 인프라, 시스템 설계 및 현장 설치를 통해 에너지 효율 및 최적화를 목표로 합니다.\n",
      "-----------------------------------\n",
      "운전 데이터 기반 에너지 진단 기술은 운전 최적화를 위한 가이드 제공을 목표로 하고, 다중 에너지 네트워크 기술은 인프라 설계 및 시스템 구축을 통해 현장 제어 및 최적화 시스템을 구축하는 것을 목표로 한다.\n",
      "-----------------------------------\n",
      "에너지 진단 및 다중 에너지 네트워크 기술은 운전 데이터 기반 최적 운전 가이드 제공과 현장 설비 제어 및 최적화 시스템 구축을 목표로 한다.\n",
      "-----------------------------------\n",
      "운전 데이터 기반 에너지 진단 기술을 통해 최적 운전 가이드를 제공하고, 다중 에너지 네트워크 기술로 인프라, 시스템 설계 및 현장 제어를 통해 에너지 효율을 극대화하는 것을 목표로 한다.\n",
      "-----------------------------------\n",
      "운전 데이터를 활용하여 에너지 효율을 높이는 최적 운전 가이드를 제공하고, 다중 에너지 네트워크를 구축하여 에너지 시스템을 설계, 제어 및 최적화합니다.\n",
      "에너지 진단 기술은 운전 데이터 분석을 통해 최적 운전을 위한 가이드를 제공하고, 다중 에너지 네트워크 기술은 설계, 구축, 운영을 통해 에너지 효율을 향상시키는 것을 목표로 합니다.\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for b in blocks:\n",
    "    [print(x) for x in b]\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:94: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:94: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\ajsj2\\AppData\\Local\\Temp\\ipykernel_12904\\2606223388.py:94: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  document = \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62050\n",
      "22\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> ## ReConPatch: Contrastive Patch Representation Learning for Industrial Anomaly Detection\n",
       "> \n",
       "> **License:** arXiv.org perpetual non-exclusive license\n",
       "> \n",
       "> **arXiv:** 2305.16713v3 [cs.CV] 10 Jan 2024\n",
       "> \n",
       "> **Authors:**\n",
       "> \n",
       "> * Jeeho Hyun\n",
       "> * Sangyun Kim\n",
       "> * Giyoung Jeon\n",
       "> * Seung Hwan Kim\n",
       "> * Kyunghoon Bae\n",
       "> * Byung Jun Kang (Correspondence: bj.kang@lgresearch.ai)\n",
       "> \n",
       "> **Affiliation:** LG AI Research\n",
       "> \n",
       "> **Abstract:**\n",
       "> \n",
       "> Anomaly detection plays a crucial role in identifying product defects such as incorrect parts, misaligned components, and damages in industrial manufacturing. Due to the scarcity of defect observations and the emergence of unknown defect types, anomaly detection remains a challenging task in machine learning. To address this issue, recent approaches leverage pre-trained visual representations from natural image datasets and extract relevant features. However, existing methods still face challenges regarding the discrepancy between the pre-trained features and the target data, or require carefully designed input augmentation, particularly for industrial datasets.\n",
       "> \n",
       "> This paper introduces **ReConPatch**, which constructs discriminative features for anomaly detection by training a linear modulation of patch features extracted from a pre-trained model. ReConPatch employs contrastive representation learning to collect and distribute features in a way that generates a target-oriented and easily separable representation. To address the lack of labeled pairs for contrastive learning, we utilize two similarity measures between data representations: pairwise and contextual similarities, as pseudo-labels.\n",
       "> \n",
       "> Our method achieves state-of-the-art anomaly detection performance (99.72%) on the widely used and challenging MVTec AD dataset. Additionally, we achieved a state-of-the-art anomaly detection performance (95.8%) on the BTAD dataset.\n",
       "> \n",
       "> **1. Introduction:**\n",
       "> \n",
       "> Anomaly detection in industrial manufacturing is crucial for identifying product defects and ensuring quality control. Anomalies can include incorrect parts, misaligned components, or product damage. Machine learning approaches for anomaly detection have gained widespread attention due to the increasing demand for automation in industrial applications. The primary focus of these approaches is to learn how to distinguish anomalies from normal cases based on collected data. However, anomaly detection presents unique challenges due to the rarity of defects and the potential for unknown defect types. This situation, where the majority of cases are labeled as normal and abnormal cases are scarce in the collected data, has driven improvements in one-class classification techniques.\n",
       "> \n",
       "> The core principle of one-class classification for anomaly detection is to train a model that learns a distance metric between data points, detecting anomalies as those located at a large distance from the nominal data. To learn this metric, reconstruction-based approaches have been proposed, aiming to detect anomalies by measuring reconstruction errors using auto-encoding models [9, 27, 34] or generative models [17, 24, 35]. These approaches strive to reconstruct normal data effectively while struggling to reconstruct anomalies, thus highlighting anomalies through reconstruction errors. However, they can be susceptible to noise and require significant data augmentation to address the issue of limited anomalous data. \n",
       "> \n",
       "> **... (Continue with the rest of the paper)**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## Unsupervised Metric Learning for Anomaly Detection with ReConPatch\n",
       "> \n",
       "> Recent work has explored using auto-encoding models [9, 27, 34] and generative adversarial networks (GANs) [29, 33] to detect anomalies by measuring reconstruction errors.  However, the limited variety of industrial data makes it challenging to establish a reliable nominal distribution.  Leveraging pre-trained models on natural image datasets [12] has shown promise for anomaly detection [3, 8], but these models struggle to discern subtle defects in industrial images due to a distribution shift between natural and industrial data.\n",
       "> \n",
       "> **Addressing the Distribution Shift**\n",
       "> \n",
       "> To mitigate the distribution shift between pre-trained and industrial datasets, researchers have proposed methods like knowledge distillation [5] and normalizing flows [13, 31]. These approaches aim to extract anomaly-specific features while leveraging pre-trained representations.  However, they still necessitate extensive hand-crafted data augmentation, which requires domain expertise and can be challenging to implement effectively for industrial images.\n",
       "> \n",
       "> **Introducing ReConPatch**\n",
       "> \n",
       "> This paper introduces ReConPatch, an unsupervised metric learning framework for anomaly detection that enhances feature arrangement without requiring data augmentation.  Contrastive learning methods often struggle to model variations within nominal instances, leading to increased false-positive rates. \n",
       "> \n",
       "> ReConPatch addresses this limitation by utilizing contextual similarity [19] among features as a pseudo-label for training. This approach allows for efficient adaptation of feature representation using only a simple linear transformation, rather than training the entire network.  This targeted feature representation improves anomaly detection accuracy without the need for input augmentation, making ReConPatch a practical and effective solution for industrial anomaly detection.\n",
       "> \n",
       "> **ReConPatch Architecture**\n",
       "> \n",
       "> ReConPatch comprises two networks:\n",
       "> \n",
       "> * **Feature representation layer (f, f¯):** Extracts patch-level features.\n",
       "> * **Projection layer (g, g¯):** Projects features into a lower-dimensional space.\n",
       "> \n",
       "> The upper networks (f¯, g¯) compute pairwise and contextual similarities between patch-level feature pairs, while the lower networks (f, g) learn the representation of patch-level features.\n",
       "> \n",
       "> **Figure 1: Overall structure of the anomaly detection using ReConPatch**\n",
       "> \n",
       "> [Insert Figure 1 here, illustrating the architecture of ReConPatch with the networks f, f¯, g, g¯ and their connections.]\n",
       "> \n",
       "> **Key Contributions**\n",
       "> \n",
       "> * **Unsupervised metric learning framework for anomaly detection:** ReConPatch enhances feature arrangement without requiring data augmentation.\n",
       "> * **Contextual similarity as a pseudo-label:** ReConPatch leverages contextual similarity among features to improve training efficiency and accuracy.\n",
       "> * **Targeted feature representation learning:** ReConPatch efficiently adapts feature representation using a simple linear transformation, leading to improved anomaly detection accuracy.\n",
       "> \n",
       "> **Conclusion**\n",
       "> \n",
       "> ReConPatch offers a practical and effective solution for anomaly detection in industrial settings. By exploiting contextual similarity and simplifying feature representation learning, ReConPatch achieves high accuracy without requiring extensive hand-crafted input augmentation.  This approach promises to improve the reliability and efficiency of anomaly detection in various industrial applications. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## ReConPatch: A Novel Anomaly Detection Method for Industrial Applications\n",
       "> \n",
       "> ### 1. Introduction\n",
       "> \n",
       "> Anomaly detection is crucial in industrial applications, where identifying deviations from normal behavior can prevent failures and optimize processes. This paper presents **ReConPatch**, a novel method for anomaly detection that leverages pre-trained models and contrastive learning to effectively identify anomalies in industrial data.\n",
       "> \n",
       "> **Key features of ReConPatch:**\n",
       "> \n",
       "> * Utilizes a pre-trained model to extract features from input data.\n",
       "> * Employs contrastive learning to learn a robust representation of normal data.\n",
       "> * Introduces a patch-based approach for improved localization of anomalies.\n",
       "> \n",
       "> **Technical details:**\n",
       "> \n",
       "> * The pre-trained model, denoted by **f**, extracts features from each patch of the input image.\n",
       "> * A contrastive learning loss, **ℒ<sub>RC</sub>**, is applied to the bottom networks **f** and **g**, which are responsible for learning patch-level features. \n",
       "> * Anomalies are detected by comparing the learned representations with those of normal data.\n",
       "> \n",
       "> ### 2. Related Work\n",
       "> \n",
       "> Unsupervised anomaly detection using neural networks has been extensively explored, with various approaches emerging. Some notable methods include:\n",
       "> \n",
       "> * **Deep Support Vector Data Description (Deep SVDD):** This method trains a neural network to map data points to a hypersphere, with anomalies defined as points lying outside the sphere [32]. Patch SVDD extends this approach to patch-level features for enhanced localization [42].\n",
       "> * **Reconstruction-based approaches:** These methods assume that normal data can be accurately reconstructed by a trained model, while anomalies exhibit reconstruction errors. Autoencoders are often employed for this purpose [9, 27, 34].\n",
       "> * **Generative Adversarial Networks (GANs):** GANs have demonstrated effectiveness in anomaly detection, particularly for generating realistic samples of normal data [29, 33].\n",
       "> * **Pre-trained models:** Leveraging pre-trained models trained on large datasets can alleviate the need for extensive training data for anomaly detection. Prior studies have used such models for anomaly detection by measuring distances between representations of input data and their nearest neighbors [3, 8]. \n",
       "> * **Normalizing flows:** DifferNet [31] proposes a normalizing flow to learn a bijective mapping between pre-trained features and the distribution of normal data, allowing for anomaly detection based on deviations from this distribution. CFLOW-AD [14] extends this concept using positional encoding.\n",
       "> * **Patch-based methods:** PatchCore [30] focuses on locally aware patch features and efficient subsampling for anomaly detection. CFA [20] trains a patch descriptor that maps features onto a hypersphere centered around the nearest neighbor in a memory bank. PaDiM [10] estimates a Gaussian distribution of patch features for anomaly localization. PNI [2] predicts the feature distribution of spatial locations and their neighborhoods.\n",
       "> \n",
       "> ### 3. Method\n",
       "> \n",
       "> Our proposed method, **ReConPatch**, focuses on...\n",
       "> \n",
       "> **(The document continues here. Please provide the remaining content for the completion of the method description.)** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## ReConPatch: Learning Target-Oriented Features for Anomaly Detection\n",
       "> \n",
       "> **3. Method**\n",
       "> \n",
       "> Our proposed method, ReConPatch, focuses on learning a representation space that maps features extracted from nominal image patches to be grouped closely if they share similar nominal characteristics in an unsupervised learning manner. Although previous work [30] has shown the effectiveness of selecting representative nominal patch features using a pre-trained model, this model still presents a representation biased to the natural image data, which has a gap with the target data. The main concept of our proposed approach is to train target-oriented features that spread out the distributions of patch features according to the variations in normal samples, and gathers similar features.\n",
       "> \n",
       "> **3.1 Overall Structure**\n",
       "> \n",
       "> As shown in Fig. 1, our framework consists of the training and the inference phases.\n",
       "> \n",
       "> **Training Phase:**\n",
       "> \n",
       "> 1. **Feature Extraction:** For each input *x* in the training data, we first collect the feature map at layer *l*,  Φ*l*(x) ∈ ℝ^C×H×W, using a pre-trained CNN model.\n",
       "> 2. **Patch-level Feature Generation:** The feature maps have different spatial resolutions at the feature hierarchy of the CNN, so they are interpolated to have the same resolution before being concatenated. Patch-level features *P*(x, h, w) ∈ ℝ^C′1 are then generated by aggregating the feature vectors of the neighborhood within a specific patch size *s* in the same approach employed in PatchCore [30]. Adaptive average pooling is used for the local aggregation.\n",
       "> 3. **Representation Learning:** ReConPatch utilizes two networks to train representations of the patch-level features:\n",
       ">     * **Representation Network:** This network is trained using the relaxed contrastive loss ℒ*R*C in Eq. 7. It is composed of a feature representation layer *f* and the projection layer *g*. Pseudo-labels are provided for every pair of features when computing ℒ*R*C.\n",
       ">     * **Similarity Calculation Network:** This network calculates pairwise and contextual similarities between patch-level feature pairs. It is gradually updated by an exponential moving average (EMA) of the representation network. Its layers are denoted as *f̄* and *ḡ*.\n",
       "> 4. **Feature Selection:** After training, patch-level features extracted from the pre-trained CNN are transformed into target-oriented features using the feature representation layer *f* [6]. The representative features are selected using the coreset subsampling approach based on the greedy approximation algorithm [36] and stored in a memory bank.\n",
       "> \n",
       "> **Inference Phase:**\n",
       "> \n",
       "> 1. **Feature Extraction:** Features of a test sample are extracted using the same process as training.\n",
       "> 2. **Anomaly Score Calculation:** The anomaly score is calculated by comparing the features with the nominal representative in the memory bank.\n",
       "> \n",
       "> **3.2 Patch-level Feature Representation Learning**\n",
       "> \n",
       "> The objective of ReConPatch is to learn target-oriented features from patch-level features, thereby enabling:\n",
       "> \n",
       "> ... \n",
       "> \n",
       "> **[Continue with the remaining details of the method and the description of the relaxed contrastive loss (ℒ*R*C)]** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## ReConPatch: Learning Target-Oriented Features from Patch-Level Representations\n",
       "> \n",
       "> The objective of ReConPatch is to learn target-oriented features from patch-level features, thereby enabling more effective discrimination between normal and abnormal features. To achieve this, a patch-level features representation learning approach is applied to aggregate highly similar features while repelling those with low similarity. However, training this approach requires labeled pairs to indicate the proximity between patch-level features.\n",
       "> \n",
       "> To address this issue, we utilize the similarity between patch-level features using the pairwise similarity and the contextual similarities as pseudo-labels. If the similarity between two features is high, the pair is pseudo-labeled as positive, and vice versa.\n",
       "> \n",
       "> ### Pairwise Similarity\n",
       "> \n",
       "> For two arbitrary patch-level features $p_i$ and $p_j$ obtained by $\\mathcal{P}(x, h, w)$, let the projected representation be $\\bar{z}_i = \\bar{g}( \\bar{f}(p_i))$ and $\\bar{z}_j = \\bar{g}( \\bar{f}(p_j))$. The pairwise similarity between the two features, $\\omega_{ij}^{Pairwise}$, is then given by:\n",
       "> \n",
       "> $\\omega_{ij}^{Pairwise} = e^{-\\frac{\\|\\bar{z}_i - \\bar{z}_j\\|^2_2}{\\sigma}}$ (1)\n",
       "> \n",
       "> where $\\sigma$ is the bandwidth of the Gaussian kernel, which can be adjusted to tune the degree of smoothing in the similarity measure [18, 19].\n",
       "> \n",
       "> ### Contextual Similarity\n",
       "> \n",
       "> **Figure 2**: Illustrative examples of similarity measures in the representation space. The pairwise similarity $\\omega_{ij}^{Pairwise}$ between $\\bar{z}_i$ and $\\bar{z}_j$ is identical in both (a) and (b). In (a), the $k$-nearest neighbors $\\mathcal{N}_k(i)$ and $\\mathcal{N}_k(j)$ do not enclose each other. Therefore, $\\omega_{ij}^{Contextual}$ has a lower value, and the $\\bar{z}_i$ and $\\bar{z}_j$ pair should be separated. By contrast, as $\\mathcal{N}_k(i)$ and $\\mathcal{N}_k(j)$ enclose each other in (b), $\\omega_{ij}^{Contextual}$ takes a higher value, so that $\\bar{z}_i$ and $\\bar{z}_j$ pair should attract each other.\n",
       "> \n",
       "> We note that Eq. 1 is used to measure the Gaussian kernel similarity between $p_i$ and $p_j$, which is widely used to measure anomaly scores. However, the pairwise similarity is insufficient to consider the relationships among groups of features. As depicted in Fig. 2, for example, cases (a) and (b) have the same pairwise similarity. In (a), $\\bar{z}_i$ and $\\bar{z}_j$ belong to different groups of features; therefore, they should be separated. By contrast, in (b), they belong to the same group and should be gathered.\n",
       "> \n",
       "> This leads to the simultaneous measure of contextual similarity, which considers the neighborhood of an embedding vector. Let the $k$-nearest neighborhood of the feature index $i$ be given as a set of indices, $\\mathcal{N}_k(i) = \\{j | d_{ij} \\leq d_{il}\\}$, where $l$ is the $k$-th nearest neighbor and $d_{ij}$ denotes the Euclidean distance between the two embedding vectors ($d_{ij} = \\|\\bar{z}_i - \\bar{z}_j\\|^2_2$). Two patch-level features can be regarded as contextually similar if they share more nearest neighbors in common [22]. The contextual similarity $\\tilde{\\omega}_{ij}^{Contextual}$ ... \n",
       "> \n",
       "> **Note:** This document is a start, and further information is needed to complete the description of the contextual similarity and how it is used within ReConPatch. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## Contextual Similarity for Patch-Level Feature Learning\n",
       "> \n",
       "> This document describes a method for calculating contextual similarity between patch-level features, which is used for training a representation learning network. \n",
       "> \n",
       "> ### Contextual Similarity Calculation\n",
       "> \n",
       "> The contextual similarity between two patch-level features **p<sub>i</sub>** and **p<sub>j</sub>** is defined as the proportion of shared nearest neighbors in their respective k-nearest neighbor sets.\n",
       "> \n",
       "> **Definition 1: Contextual Similarity**\n",
       "> \n",
       "> ```\n",
       "> ω<sub>ij</sub><sup>Contextual</sup> = \n",
       "> { \n",
       ">   |𝒩<sub>k</sub>(i) ∩ 𝒩<sub>k</sub>(j)| / |𝒩<sub>k</sub>(i)|,  if j ∈ 𝒩<sub>k</sub>(i)\n",
       ">   0, otherwise \n",
       "> }\n",
       "> (2)\n",
       "> ```\n",
       "> \n",
       "> where:\n",
       "> \n",
       "> * **𝒩<sub>k</sub>(i)** represents the set of k-nearest neighbors of patch **p<sub>i</sub>**.\n",
       "> * **|A|** denotes the cardinality of set A.\n",
       "> \n",
       "> This similarity is then refined using **query expansion**, which considers neighbors of neighbors.\n",
       "> \n",
       "> **Definition 2: Reciprocal k-Nearest Neighbors**\n",
       "> \n",
       "> ```\n",
       "> ℛ<sub>k</sub>(i) = { j | j ∈ 𝒩<sub>k</sub>(i) and i ∈ 𝒩<sub>k</sub>(j) }\n",
       "> (3)\n",
       "> ```\n",
       "> \n",
       "> **Definition 3: Refined Contextual Similarity**\n",
       "> \n",
       "> ```\n",
       "> ω<sup>^</sup><sub>ij</sub><sup>Contextual</sup> = 1/|ℛ<sub>k</sub><sup>2</sup>(i)| * Σ<sub>l∈ℛ<sub>k</sub><sup>2</sup>(i)</sub> ω<sub>lj</sub><sup>Contextual</sup>\n",
       "> (4)\n",
       "> ```\n",
       "> \n",
       "> This refined similarity is calculated by averaging the contextual similarities over the set of reciprocal k-nearest neighbors.\n",
       "> \n",
       "> **Definition 4: Final Contextual Similarity**\n",
       "> \n",
       "> ```\n",
       "> ω<sub>ij</sub><sup>Contextual</sup> = 1/2 * (ω<sup>^</sup><sub>ij</sub><sup>Contextual</sup> + ω<sup>^</sup><sub>ji</sub><sup>Contextual</sup>)\n",
       "> (5)\n",
       "> ```\n",
       "> \n",
       "> The final contextual similarity is defined as the average of the bi-directional refined contextual similarities.\n",
       "> \n",
       "> ### Combining Patch-Level and Contextual Similarities\n",
       "> \n",
       "> The overall similarity between two patch-level features is defined as a linear combination of the patch-level similarity (**ω<sub>ij</sub><sup>Pairwise</sup>**) and the contextual similarity (**ω<sub>ij</sub><sup>Contextual</sup>**).\n",
       "> \n",
       "> **Definition 5: Final Similarity**\n",
       "> \n",
       "> ```\n",
       "> ω<sub>ij</sub> = α * ω<sub>ij</sub><sup>Pairwise</sup> + (1 - α) * ω<sub>ij</sub><sup>Contextual</sup>\n",
       "> (6)\n",
       "> ```\n",
       "> \n",
       "> where **α ∈ [0, 1]** is a weighting factor.\n",
       "> \n",
       "> ### Relaxed Contrastive Loss\n",
       "> \n",
       "> The relaxed contrastive loss is used to train the representation learning networks. It considers inter-feature similarity as pseudo-labels and encourages similar features to be close in embedding space while pushing dissimilar features apart.\n",
       "> \n",
       "> **Definition 6: Relaxed Contrastive Loss**\n",
       "> \n",
       "> ```\n",
       "> ℒ<sup>RC</sup>(z) = 1/N * Σ<sub>i=1</sub><sup>N</sup> Σ<sub>j=1</sub><sup>N</sup> ω<sub>ij</sub> δ<sub>ij</sub><sup>2</sup> + (1 - ω<sub>ij</sub>) max(m - δ<sub>ij</sub>, 0)<sup>2</sup>\n",
       "> (7)\n",
       "> ```\n",
       "> \n",
       "> where:\n",
       "> \n",
       "> * **z** represents the embedding vectors.\n",
       "> * **δ<sub>ij</sub> = ||z<sub>i</sub> - z<sub>j</sub>||<sup>2</sup> / (1/N * Σ<sub>n=1</sub><sup>N</sup> ||z<sub>i</sub> - z<sub>n</sub>||<sup>2</sup>)** is the relative distance between embedding vectors.\n",
       "> * **N** is the number of instances in a mini-batch.\n",
       "> * **m** is the repelling margin.\n",
       "> \n",
       "> ### Training the Similarity Calculation Network\n",
       "> \n",
       "> The similarity calculation network (**f<sup>-</sup>** and **g<sup>-</sup>**) is updated using an exponential moving average (EMA) of the parameters of the representation learning network (**f** and **g**). This approach helps stabilize training by ensuring consistency in the relationships between patch-level features.\n",
       "> \n",
       "> **Definition 7: EMA Update**\n",
       "> \n",
       "> ```\n",
       "> θ<sub>f<sup>-</sup>, g<sup>-</sup></sub> ← γ * θ<sub>f<sup>-</sup>, g<sup>-</sup></sub> + (1 - γ) * θ<sub>f, g</sub>\n",
       "> ```\n",
       "> \n",
       "> where:\n",
       "> \n",
       "> * **θ<sub>f<sup>-</sup>, g<sup>-</sup></sub>** represents the parameters of the similarity calculation network.\n",
       "> * **θ<sub>f, g</sub>** represents the parameters of the representation learning network.\n",
       "> * **γ** is the EMA decay rate.\n",
       "> \n",
       "> ### Conclusion\n",
       "> \n",
       "> This document describes a method for calculating contextual similarity between patch-level features, which is used for training a representation learning network. The approach combines patch-level and contextual similarities, utilizes a relaxed contrastive loss function, and employs an EMA for updating the similarity calculation network. This framework allows for learning robust and meaningful representations of patch-level features. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## ReConPatch: Reconstructing Patch Features for Anomaly Detection\n",
       "> \n",
       "> This document outlines the ReConPatch method for anomaly detection, focusing on its core features and evaluation.\n",
       "> \n",
       "> ### 3.3 Anomaly Detection with ReConPatch\n",
       "> \n",
       "> **Anomaly Score Calculation:**\n",
       "> \n",
       "> ReConPatch calculates anomaly scores similar to PatchCore [30]. After training, the coreset ℳ is extracted from the newly trained feature representation 𝑓(⋅) using a greedy approximation algorithm [36]. This coreset serves as a representative feature set.\n",
       "> \n",
       "> The anomaly score for a patch 𝑝𝑡 is calculated as follows:\n",
       "> \n",
       "> 1. Find the nearest coreset element 𝑟* to the patch's representation 𝑓(𝑝𝑡) in the memory bank:\n",
       "> \n",
       "> ```\n",
       "> 𝑟* = argmin_𝑟 ∈ ℳ 𝒟(𝑓(𝑝𝑡), 𝑟)\n",
       "> ```\n",
       "> \n",
       "> 2. Calculate the pixel-wise anomaly score 𝑠𝑡:\n",
       "> \n",
       "> ```\n",
       "> 𝑠𝑡 = (1 - 𝑒^𝑠𝑡′ ∑_𝑟′ ∈ 𝒩𝑏(𝑟*) 𝑒^𝒟(𝑓(𝑝𝑡), 𝑟′)) 𝒟(𝑓(𝑝𝑡), 𝑟*)\n",
       "> ```\n",
       "> \n",
       "> where:\n",
       "> \n",
       "> - 𝒩𝑏(𝑟*) represents the set of 𝑏-nearest neighbors of 𝑟* in the memory bank.\n",
       "> - 𝒟 represents the distance metric used.\n",
       "> \n",
       "> 3. Calculate the image-wise anomaly score as the maximum of the patch-wise scores.\n",
       "> \n",
       "> **Score-Level Ensemble:**\n",
       "> \n",
       "> To enhance accuracy, ReConPatch utilizes score-level ensemble from multiple models. Scores are normalized using the modified z-score [1] to compensate for different score distributions:\n",
       "> \n",
       "> ```\n",
       "> 𝑠¯𝑡 = 𝑠𝑡 - 𝑠~ 𝛽 ⋅ 𝑀𝐴𝐷\n",
       "> ```\n",
       "> \n",
       "> where:\n",
       "> \n",
       "> - 𝑠~ represents the median value of anomaly scores over the entire training dataset.\n",
       "> - 𝑀𝐴𝐷 is the Mean Absolute Deviation over the entire training dataset.\n",
       "> - 𝛽 is a constant scale factor set to 1.4826, assuming a normal distribution of anomaly scores.\n",
       "> \n",
       "> ### 4 Experiments and Analysis\n",
       "> \n",
       "> **4.1 Experimental Setup**\n",
       "> \n",
       "> **Datasets:**\n",
       "> \n",
       "> - **MVTec AD [4]:** A widely used benchmark for industrial anomaly detection, containing 15 categories with 3,629 training and 1,725 test images.\n",
       "> - **BTAD [26]:** A dataset with RGB images representing three industrial products, consisting of 1,799 training and 741 test images.\n",
       "> \n",
       "> **Metrics:**\n",
       "> \n",
       "> - **Area Under the Receiver Operating Characteristic (AUROC):** Used to evaluate both anomaly detection and segmentation performance following [8, 10, 20, 30].\n",
       ">     - **Image-level AUROC:** Evaluates detection performance based on image-level anomaly scores and normal/abnormal labels.\n",
       ">     - **Pixel-level AUROC:** Evaluates segmentation performance based on pixel-wise anomaly scores and ground truth masks.\n",
       "> \n",
       "> **Please note:** The provided text appears to be incomplete. The section on experiments and analysis ends abruptly. \n",
       "> \n",
       "> Please provide the remaining text for a complete and comprehensive Markdown document. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## Reconstructing Patch Features for Anomaly Detection and Segmentation\n",
       "> \n",
       "> This document presents an ablation study of the Reconstructing Patch Features (ReConPatch) model for anomaly detection and segmentation. \n",
       "> \n",
       "> ### Ablation Study Results\n",
       "> \n",
       "> **Table 1: Coreset Subsampling Percentage Ablation Study**\n",
       "> \n",
       "> | Method | Ours-25% | Ours-10% | Ours-1% |\n",
       "> |---|---|---|---|\n",
       "> | Detection | 99.24 | 99.27 | 99.49 |\n",
       "> | Segmentation | 98.01 | 98.07 | 98.07 |\n",
       "> \n",
       "> *This table shows the performance of the ReConPatch model with a WideResNet-50 backbone on the MVTec AD dataset, varying the coreset subsampling percentage. The best performance is achieved with 1% coreset subsampling.*\n",
       "> \n",
       "> **Table 2: f Layer Dimension Ablation Study**\n",
       "> \n",
       "> | Dimension | 1024 | 512 | 256 | 128 | 64 |\n",
       "> |---|---|---|---|---|---|\n",
       "> | PatchCore | 99.1 | 98.66 | 98.45 | 98.54 | 97.75 |\n",
       "> | ReConPatch | 99.49 | 99.56 | 99.53 | 99.52 | 99.14 |\n",
       "> \n",
       "> *This table compares the performance of PatchCore and ReConPatch models with varying f layer dimensions on the MVTec AD dataset using a WideResNet-50 backbone. ReConPatch consistently outperforms PatchCore across all dimensions, with the best performance at 512 dimension.*\n",
       "> \n",
       "> **Table 3: Hierarchy Levels and Patch Size Ablation Study**\n",
       "> \n",
       "> | Metric | Detection | Segmentation |\n",
       "> |---|---|---|\n",
       "> | WRN-50, s=3, 512 dim, layer (2+3), Imagesize 224 | AUROC: 99.56 | AUROC: 98.07 |\n",
       "> | WRN-50, s=5, 512 dim, layer (2+3), Imagesize 224 | AUROC: 98.84 | AUROC: 97.82 |\n",
       "> | WRN-50, s=5, 512 dim, layer (1+2+3), Imagesize 224 | AUROC: 98.7 | AUROC: 98.18 |\n",
       "> \n",
       "> *This table investigates the effect of adding more hierarchy levels and increasing patch size on the ReConPatch model. The best performance is achieved with s=5 and layers (1+2+3) for segmentation, while s=3 and layers (2+3) work best for detection.*\n",
       "> \n",
       "> **Table 4: Data Augmentation Ablation Study**\n",
       "> \n",
       "> | Method | Class → ↓ Aug. Method | Object | Texture | Average |\n",
       "> |---|---|---|---|---|\n",
       "> | w/o Aug. |  | 99.17 | 98.96 | 99.10 |\n",
       "> | PatchCore | w/ Aug. | 94.86 | 96.09 | 95.48 |\n",
       "> | Diff. |  | 9.94 | 2.87 | 3.62 |\n",
       "> | w/o Aug. |  | 99.44 | 99.81 | 99.56 |\n",
       "> | ReConPatch | w/ Aug. | 97.65 | 99.47 | 98.56 |\n",
       "> | Diff. |  | 1.79 | 0.34 | 1.00 |\n",
       "> \n",
       "> *This table shows the impact of data augmentation on PatchCore and ReConPatch models. ReConPatch demonstrates significantly better robustness to data augmentation compared to PatchCore, suggesting its better ability to capture meaningful features.*\n",
       "> \n",
       "> ### Implementation Details\n",
       "> \n",
       "> **Single Model**\n",
       "> \n",
       "> * Feature extractor: ImageNet pre-trained WideResNet-50\n",
       "> * f layer output size: 512\n",
       "> * Coreset subsampling percentage: 1%\n",
       "> * Training epochs: 120 per category\n",
       "> * Hierarchy levels: 2 and 3 (unless specified)\n",
       "> * Patch size: s=3 (unless specified)\n",
       "> \n",
       "> **Ensemble Model**\n",
       "> \n",
       "> * Feature extractors: WideResNet-101, ResNext-101, DenseNet-201\n",
       "> * f layer output size: 384\n",
       "> * Coreset subsampling percentage: 1%\n",
       "> * Training epochs: 60 per category\n",
       "> * Hierarchy levels: 2 and 3\n",
       "> * Patch size: s=3 (unless specified)\n",
       "> \n",
       "> **Comparison with PNI [2]**\n",
       "> \n",
       "> * Feature extractor: WideResNet-101\n",
       "> * Hierarchy levels: 2 and 3\n",
       "> * Patch size: s=5 \n",
       "> * Training epochs: 120 per category\n",
       "> \n",
       "> **Segmentation Evaluation**\n",
       "> \n",
       "> * Hierarchy levels: 1, 2, and 3\n",
       "> * Patch size: s=5\n",
       "> \n",
       "> These implementation details provide a comprehensive understanding of the training and evaluation processes used for ReConPatch. Further details and analyses can be found in the relevant research paper. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## ReConPatch: Anomaly Detection and Segmentation with Reconstructive Patch-based Anomaly Detection\n",
       "> \n",
       "> **Abstract:**\n",
       "> This document provides insights into ReConPatch, a method for anomaly detection and segmentation. It delves into the model's training process, hyperparameters, and performance on the MVTec AD dataset.\n",
       "> \n",
       "> **Training Setup:**\n",
       "> \n",
       "> * **Batch size:** 𝑠 = 5\n",
       "> * **Epochs per category:** 120\n",
       "> * **Optimizer:** AdamP [15]\n",
       "> * **Scheduler:** Cosine annealing [23]\n",
       "> * **Learning rate:** \n",
       ">     * Single model: 1e-5\n",
       ">     * Ensemble model: 1e-6\n",
       "> * **Weight decay:** 1e-2\n",
       "> * **Learning rate for models with 480x480 image size:** 1e-6\n",
       "> \n",
       "> **Hyperparameter details are available in Appendix B.**\n",
       "> \n",
       "> **Backbone Architectures:**\n",
       "> \n",
       "> | Backbone   | WRN-101 | WRN-50 |\n",
       "> |-----------|---------|--------|\n",
       "> | Image size | 480x480 | 480x480 |\n",
       "> | Image size | 256x256 | 224x224 |\n",
       "> | Image size | 224x224 | 224x224 |\n",
       "> | Image size | 224x224 | 224x224 |\n",
       "> | Image size | 224x224 | 224x224 |\n",
       "> \n",
       "> **Performance on MVTec AD dataset:**\n",
       "> \n",
       "> | Class\\Method → | PNI [2] (w/ refine) | Ours | CFLOW-AD [14] | SPADE [8] | PaDiM [10] | PatchCore [30] | CFA [20] | Ours |\n",
       "> |---|---|---|---|---|---|---|---|---|\n",
       "> | Bottle | (100, 98.87) | (100, 98.78) | (100, 98.76) | (-, 98.4) | (-, 98.3) | (100, 98.6) | (100, -) | (100, 98.2) |\n",
       "> | Cable | (99.76, 99.1) | (99.66, 98.86) | (97.59, 97.64) | (-, 97.2) | (-, 96.7) | (99.5, 98.4) | (99.8, -) | (99.83, 99.3) |\n",
       "> | Capsule | (99.72, 99.34) | (99.76, 99.24) | (97.68, 98.98) | (-, 99) | (-, 98.5) | (98.1, 98.8) | (97.3, -) | (98.8, 97.61) |\n",
       "> | Hazelnut | (100, 99.37) | (100, 99.07) | (99.98, 98.82) | (-, 99.1) | (-, 98.2) | (100, 98.7) | (100, -) | (100, 98.94) |\n",
       "> | Metal nut | (100, 99.29) | (100, 99.29) | (99.26, 98.56) | (-, 98.1) | (-, 97.2) | (100, 98.4) | (100, -) | (100, 95.76) |\n",
       "> | Pill | (96.89, 99.03) | (96.21, 98.66) | (96.82, 98.95) | (-, 96.5) | (-, 95.7) | (96.6, 97.4) | (97.9, -) | (97.49, 95.35) |\n",
       "> | Screw | (99.51, 99.6) | (99.84, 99.59) | (91.89, 98.1) | (-, 98.9) | (-, 98.5) | (98.1, 99.4) | (97.3, -) | (98.52, 98.79) |\n",
       "> | Toothbrush | (99.72, 99.09) | (100, 99.16) | (99.65, 98.56) | (-, 97.9) | (-, 98.8) | (100, 98.7) | (100, -) | (100, 98.88) |\n",
       "> | Transistor | (100, 98.04) | (100, 96.18) | (95.21, 93.28) | (-, 94.1) | (-, 97.5) | (100, 96.3) | (100, -) | (100, 99.65) |\n",
       "> | Zipper | (99.87, 99.43) | (99.89, 99.25) | (98.48, 98.41) | (-, 96.5) | (-, 98.5) | (99.4, 98.8) | (99.6, -) | (99.76, 98.56) |\n",
       "> | Object classes | (99.55, 99.12) | (99.54, 98.81) | (97.66, 98.01) | (-, 97.57) | (-, 97.79) | (99.17, 98.35) | (99.19, -) | (99.44, 98.1) |\n",
       "> | Carpet | (100, 99.4) | (100, 99.29) | (98.73, 99.23) | (-, 97.5) | (-, 99.1) | (98.7, 99) | (97.3, -) | (99.6, 98.75) |\n",
       "> | Grid | (98.41, 99.2) | (99.5, 98.73) | (99.6, 96.89) | (-, 93.7) | (-, 97.3) | (98.2, 98.7) | (99.2, -) | (100, 99.04) |\n",
       "> | Leather | (100, 99.56) | (100, 99.48) | (100, 99.61) | (-, 97.6) | (-, 99.2) | (100, 99.3) | (100, -) | (100, 96.02) |\n",
       "> | Tile | (100, 98.4) | (100, 97.15) | (99.88, 97.71) | (-, 87.4) | (-, 94.1) | (98.7, 95.6) | (99.4, -) | (99.78, 98.92) |\n",
       "> | Wood | (99.56, 97.04) | (99.47, 95.16) | (99.12, 94.49) | (-, 88.5) | (-, 94.9) | (99.2, 95) | (99.7, -) | (99.65, 98.9) |\n",
       "> | Texture classes | (99.59, 98.72) | (99.79, 97.96) | (99.47, 97.59) | (-, 92.94) | (-, 96.92) | (98.96, 97.52) | (99.12, -) | (99.81, 98.33) |\n",
       "> | Average | (99.56, 98.98) | (99.62, 98.53) | (98.26, 97.87) | (85.5, 96) | (95.3, 97.5) | (99.1, 98.1) | (99.2, 98.2) | (99.56, 98.18) |\n",
       "> \n",
       "> **Table 5: Anomaly detection and segmentation performance on the MVTec AD dataset. (image-level AUROC, pixel-level AUROC)**\n",
       "> \n",
       "> **4.2 Ablation study:**\n",
       "> \n",
       "> This section investigates the optimal configuration of ReConPatch through ablation studies. The primary focus is on determining the ideal coreset subsampling percentage for anomaly detection and segmentation performance. \n",
       "> \n",
       "> **Please note:** The text provided is incomplete and ends abruptly. More details on the ablation study are needed to complete the document. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## ReConPatch: A Robust Patch-Level Anomaly Detection and Segmentation Method\n",
       "> \n",
       "> This document summarizes the performance of ReConPatch, a novel approach for anomaly detection and segmentation, and compares it to existing methods.\n",
       "> \n",
       "> ### Subsampling Percentage and Feature Dimension\n",
       "> \n",
       "> The performance of ReConPatch is evaluated using different subsampling percentages and feature dimensions. The following points are highlighted:\n",
       "> \n",
       "> * **Subsampling Percentage:** \n",
       ">     * Three subsampling percentages (25%, 10%, and 1%) were used, following the approach in PatchCore [30].\n",
       ">     * The 1% subsampling percentage yielded the best performance.\n",
       "> * **Feature Dimension:**\n",
       ">     * Experiments were conducted with varying output dimensions of the `f` layer (1024, 512, 256, 128, and 64) using a 1% coreset subsampling.\n",
       ">     * The dimension of 512 achieved the highest performance.\n",
       ">     * Even with a 64-dimensional feature space, ReConPatch outperformed PatchCore with 1024 dimensions, showcasing the method's ability to reduce dimensionality while maintaining performance.\n",
       "> \n",
       "> **Table 1:** Performance with different subsampling percentages.\n",
       "> \n",
       "> **Table 2:** Performance with different feature dimensions.\n",
       "> \n",
       "> ### Ablation Study: Hierarchy Levels and Patch Size\n",
       "> \n",
       "> An ablation study was conducted using the MVTec AD dataset [4] to investigate the impact of varying hierarchy levels and patch sizes on segmentation performance.\n",
       "> \n",
       "> * **Results:** Increasing the patch size to 5 and utilizing hierarchy levels 1, 2, and 3 resulted in a segmentation performance improvement of up to 98.18%, with a minor decrease in detection performance.\n",
       "> \n",
       "> **Table 3:** Ablation study results with varying hierarchy levels and patch sizes.\n",
       "> \n",
       "> ### Robustness to Environmental Changes\n",
       "> \n",
       "> ReConPatch's robustness to real-world environmental changes was assessed by introducing random transformations like rotation, translation, color jitter, and Gaussian blur.\n",
       "> \n",
       "> * **Results:** ReConPatch showed minimal performance degradation (image-level AUROC decreased to 1.0) compared to PatchCore, which experienced a significant drop (image-level AUROC decreased to 3.62) under the same conditions.\n",
       "> \n",
       "> **Table 4:** Performance under simulated real-world conditions.\n",
       "> \n",
       "> ### Ensemble Model Performance\n",
       "> \n",
       "> An ensemble model was created using multiple backbones (WRN-101, RNext-101, and DenseN-201) and different image sizes (480x480 and 320x320).\n",
       "> \n",
       "> **Table 5:** Comparison of ensemble model performance with different backbones and image sizes.\n",
       "> \n",
       "> **Table 6:** Comparison of ensemble model performance with other state-of-the-art methods on the MVTec AD dataset.\n",
       "> \n",
       "> **Table 7:** Comparison of performance across different anomaly detection and segmentation methods on specific classes.\n",
       "> \n",
       "> ### Conclusion\n",
       "> \n",
       "> ReConPatch demonstrates superior performance compared to existing methods in anomaly detection and segmentation tasks. Its robustness to environmental changes, capability for dimension reduction, and improved segmentation performance through hierarchical features make it a valuable tool for real-world applications.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## Anomaly Detection and Segmentation Performance\n",
       "> \n",
       "> **Table 7: Anomaly Detection and Segmentation Performance on the BTAD [26] Dataset. (image-level AUROC, pixel-level AUROC)**\n",
       "> \n",
       "> |  | (82.6, 77) | (99.9, 99.1) | (99.4, 98.8) | (91.1, 98.3) | (99.3, 98.1) | (99, 98.6) | (100, 99.7) | (100, 99.7) | (-, 99) | (99.8, 99.1) | (100, 99) |\n",
       "> |---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "> | 96.6) |  |  |  |  |  |  |  |  |  |  |  |\n",
       "> | Avg. | (83.7, 90) | (87.6, 96.9) | (93.7, 97.3) | (91, 96.3) | (95.8, 97.7) | (94.2, 96.8) | (94.3, 97.7) | (95.6, 97.4) | (-, 97.8) | (93.1, 97.3) | (95.8, 97.5) |\n",
       "> \n",
       "> **4.3 Anomaly Detection on MVTec AD**\n",
       "> \n",
       "> This section evaluates the anomaly detection performance of the proposed method on the MVTec AD dataset, comparing it with previous works using the same pre-trained model and image size [8, 10, 20, 30]. The performance of concurrent methods PNI [2] and CFLOW-AD [14] is also included in Tables 5.\n",
       "> \n",
       "> * **PNI [2]** utilized a WideResNet-101 model with an image size of 480×480. To enhance performance, a refinement network was trained in a supervised manner using artificially created defect datasets.\n",
       "> \n",
       "> * **CFLOW-AD [14]** used a WideResNet-50 model with an image size of 256×256. The evaluation results used in CFLOW-AD represent the best performance obtained for each category with an image size of 256×256.\n",
       "> \n",
       "> **Pre-processing:**\n",
       "> \n",
       "> * **Single-model performance comparison:** Images were resized to 256×256 and then center-cropped to 224×224 following the pre-processing described in previous work [8, 10, 20, 30].\n",
       "> \n",
       "> * **Ensemble model:** The same pre-processing as in [30] was applied, resizing each image to 366×366 and then center-cropping to 320×320. Additionally, for comparison with PNI [2], images were resized to 512×512 and then center-cropped to 480×480.\n",
       "> \n",
       "> * **No data augmentation was applied to any category.**\n",
       "> \n",
       "> **Performance:**\n",
       "> \n",
       "> * **ReConPatch performance in Tables 5** was obtained using 1% coreset subsampling and `f` layer dimensions of 512, determined according to Table 2.\n",
       "> \n",
       "> * **Table 5 compares the anomaly detection and segmentation performance of a single model for each category of the MVTec AD [4] dataset, evaluated with image-level AUROC.**\n",
       "> \n",
       ">     * ReConPatch achieved an image-level AUROC of 99.56%, surpassing CFA [20] (at 99.3%).\n",
       "> \n",
       ">     * ReConPatch also outperformed the state-of-the-art PNI [2] with WideResNet-101 [44], which achieved a performance of 99.62%.\n",
       "> \n",
       "> * **Focus on improving anomaly detection performance resulted in a potentially lower segmentation performance.**\n",
       "> \n",
       ">     * However, the achieved segmentation performance of 98.18% exceeded that of PatchCore [30], indicating that the addition of the ReConPatch feature in the `f` layer contributed to improved segmentation performance.\n",
       "> \n",
       "> **Refer to caption:**\n",
       "> \n",
       "> * **Figure 3: An illustrative comparison of features mapped by (a) PatchCore and (b) (c) (d) ReConPatch using the MVTec AD dataset.** The scatter plot describes the feature space of each method, colored according to the pixel position.\n",
       "> \n",
       "> * **Figure 4: The histogram of the anomaly score of the normal and abnormal samples.**\n",
       "> \n",
       "> This well-formatted Markdown document includes the provided text, restructured into tables, sections, and bullet points for clarity and readability. It also incorporates relevant information from the text about the dataset, pre-processing steps, and performance evaluation.  Please note that the referenced tables and figures are not included as they were not provided in the initial text. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## ReConPatch: Reconfigurable Feature Space for Anomaly Detection and Segmentation\n",
       "> \n",
       "> ### 4.3 Performance Evaluation on MVTec AD Dataset\n",
       "> \n",
       "> Table 6 presents the performance of our ensemble model, evaluated using the modified z-score in Eq. 11 for each output from WideResNet-101 [44], ResNext-101 [40], and DenseNet-201 [16] models. Our model achieved state-of-the-art performance in anomaly detection with an AUROC of 99.72% on the MVTec AD dataset using an image size of 480 × 480. Notably, our model outperforms PNI [2] even with a smaller image size of 320 × 320, achieving an AUROC of 99.67% compared to 99.63%. Furthermore, we surpassed PatchCore [30] in terms of anomaly segmentation performance, with an improved AUROC of 98.36%.\n",
       "> \n",
       "> **Figure 4:** The histogram of the anomaly score of the normal and abnormal data for the bottle class. ReConPatch shows high discriminability, as shown in the 𝑑′ measure.\n",
       "> \n",
       "> **Figure 5:** Examples of images with anomalies (top) and measured anomaly score maps (bottom) on the MVTec AD dataset. The orange line depicts the ground truth of the anomalies, and the green line depicts thresholds optimizing F1 scores of anomaly segmentation. The green star indicates the maximal location of the anomaly score in the heatmap.\n",
       "> \n",
       "> ### 4.4 Anomaly Detection on BTAD\n",
       "> \n",
       "> To verify the capability of anomaly detection and segmentation in other datasets, we compared the performance of our model with contemporary methods using the BTAD dataset [26]. For BTAD, we used the pre-trained WideResNet-101 model as a feature extractor and an image size of 480 × 480 for ReConPatch, achieving our best performance. Table 7 shows the image-level AUROC and the pixel-level AUROC on the BTAD dataset. Our model achieved state-of-the-art performance in anomaly detection, with an AUROC of 95.8%. Additionally, in anomaly segmentation, our model outperformed PatchCore [30] with a higher AUROC of 97.5%.\n",
       "> \n",
       "> ### 4.5 Qualitative Analysis\n",
       "> \n",
       "> To assess the impact of ReConPatch learning on the feature space, we contrasted the feature space of PatchCore and ReConPatch using the MVTec AD dataset. Our visualization, depicted in Figure 3, employed UMAP [25] for effective 2D representation of high-dimensional patch features, with color coding indicating spatial positions. The visualization attests that ReConPatch’s training encourages proximity of features with similar positions. Building on findings in prior research [2, 14], which demonstrated the value of positional information, we hypothesize that ReConPatch’s performance enhancement arises from implicit positional information learning. We also visualize the feature map along the training, which indicates the features are trained to map similar positions together.\n",
       "> \n",
       "> ReConPatch’s reconfigured feature space yields more distinct histogram distributions of image-level anomaly scores compared to PatchCore. In Figure 4, we observe this effect on the MVTec AD dataset’s bottle class. ReConPatch compresses the score distribution for normal data while pushing the abnormal data’s distribution further from the normal. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## ReConPatch: Target-Oriented Representation Learning for Anomaly Detection\n",
       "> \n",
       "> This paper presents ReConPatch, a novel method for anomaly detection based on target-oriented representation learning. ReConPatch utilizes contrastive learning with two similarity-based pseudo soft labels to effectively train a representation space that distinguishes anomalies from normal data.\n",
       "> \n",
       "> ### 1. Introduction\n",
       "> \n",
       "> Traditional anomaly detection methods often struggle with separating abnormal data from normal data. ReConPatch addresses this challenge by pushing the distributions of abnormal and normal data further apart, effectively enhancing their separability.\n",
       "> \n",
       "> ### 2. Method\n",
       "> \n",
       "> #### 2.1. Distribution Separability\n",
       "> \n",
       "> The paper quantifies distribution separability using the d' discriminability index:\n",
       "> \n",
       "> ```\n",
       "> d' = |𝜇_abnormal - 𝜇_normal| / (𝜎_abnormal^2 + 𝜎_normal^2) / 2 \n",
       "> ```\n",
       "> \n",
       "> where:\n",
       "> \n",
       "> * 𝜇_abnormal and 𝜇_normal are the means of the abnormal and normal distributions, respectively.\n",
       "> * 𝜎_abnormal and 𝜎_normal are the standard deviations of the abnormal and normal distributions, respectively.\n",
       "> \n",
       "> #### 2.2. ReConPatch Architecture\n",
       "> \n",
       "> ReConPatch utilizes patch-level features similar to those used in PatchCore. However, ReConPatch incorporates target-oriented features through patch-level representation training, which enhances discrimination between normal and abnormal attributes.\n",
       "> \n",
       "> ### 3. Results\n",
       "> \n",
       "> #### 3.1. Anomaly Score Maps\n",
       "> \n",
       "> The paper presents anomaly score maps overlaid on input images, with higher values indicating probable anomalies. These maps demonstrate ReConPatch's ability to accurately pinpoint anomaly locations.\n",
       "> \n",
       "> #### 3.2. Performance Evaluation\n",
       "> \n",
       "> ReConPatch achieves an image-level AUROC of 99.56 on the MVTec anomaly detection dataset, significantly outperforming other methods. It also performs well on the BTAD dataset.\n",
       "> \n",
       "> ### 4. Conclusion\n",
       "> \n",
       "> ReConPatch offers a novel approach to anomaly detection by learning a target-oriented representation space that effectively distinguishes anomalies from normal data. Its strengths include:\n",
       "> \n",
       "> * High performance without extensive data augmentation.\n",
       "> * Dimension reduction without significant loss of performance.\n",
       "> * Robustness to noise and intricate ground truth scenarios.\n",
       "> \n",
       "> ### 5. Future Work\n",
       "> \n",
       "> The authors plan to improve pixel-level anomaly detection by considering the correlation among neighboring features.\n",
       "> \n",
       "> ### References\n",
       "> \n",
       "> [1] Vaibhav Aggarwal, Vaibhav Gupta, Prayag Singh, Kiran Sharma, and Neetu Sharma.Detection of spatial outlier by using improved z-score test.pages 788–790, 2019.\n",
       "> \n",
       "> [2] Jaehyeok Bae, Jae-Han Lee, and Seyun Kim.Pni: industrial anomaly detection using position and neighborhood information.pages 6373–6383, 2023.\n",
       "> \n",
       "> [3] Liron Bergman, Niv Cohen, and Yedid Hoshen.Deep nearest neighbor anomaly detection.arXiv preprint arXiv:2002.10445, 2020.\n",
       "> \n",
       "> [4] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger.Mvtec ad–a comprehensive real-world dataset for unsupervised anomaly detection.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 124–132, 2019. \n",
       "> \n",
       "> **Note:** This is a formatted version of the provided text.  The references have been listed for completeness, but further research is recommended for their full context and relevance. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## References\n",
       "> \n",
       "> **[5]** Steger, Carsten. \"Mvtec ad–a comprehensive real-world dataset for unsupervised anomaly detection.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 9592–9600, 2019.\n",
       "> \n",
       "> **[6]** Bergmann, Paul, et al. \"Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 4183–4192, 2020.\n",
       "> \n",
       "> **[7]** Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" *International Conference on Machine Learning*, pp. 1597–1607. PMLR, 2020.\n",
       "> \n",
       "> **[8]** Cimpoi, Mircea, et al. \"Describing textures in the wild.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pp. 3606–3613, 2014.\n",
       "> \n",
       "> **[9]** Cohen, Niv, and Yedid Hoshen. \"Sub-image anomaly detection with deep pyramid correspondences.\" *arXiv preprint arXiv:2005.02357*, 2020.\n",
       "> \n",
       "> **[10]** Davletshina, Diana, et al. \"Unsupervised anomaly detection for x-ray images.\" *arXiv preprint arXiv:2001.10883*, 2020.\n",
       "> \n",
       "> **[11]** Defard, Thomas, et al. \"Padim: a patch distribution modeling framework for anomaly detection and localization.\" *Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10–15, 2021, Proceedings, Part IV*, pp. 475–489. Springer, 2021.\n",
       "> \n",
       "> **[12]** Deng, Hanqiu, and Xingyu Li. \"Anomaly detection via reverse distillation from one-class embedding.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 9737–9746, 2022.\n",
       "> \n",
       "> **[13]** Deng, Jia, et al. \"Imagenet: A large-scale hierarchical image database.\" *2009 IEEE Conference on Computer Vision and Pattern Recognition*, pp. 248–255. IEEE, 2009.\n",
       "> \n",
       "> **[14]** Dinh, Laurent, et al. \"Density estimation using real nvp.\" *5th International Conference on Learning Representations*, 2017.\n",
       "> \n",
       "> **[15]** Gudovskiy, Denis, et al. \"Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows.\" *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pp. 98–107, 2022.\n",
       "> \n",
       "> **[16]** Heo, Byeongho, et al. \"Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights.\" *arXiv preprint arXiv:2006.08217*, 2020.\n",
       "> \n",
       "> **[17]** Huang, Gao, et al. \"Densely connected convolutional networks.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pp. 4700–4708, 2017.\n",
       "> \n",
       "> **[18]** Johnson, Jeff, et al. \"Billion-scale similarity search with gpus\" \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## References\n",
       "> \n",
       "> [17] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. *IEEE Transactions on Big Data*, pages 1–1, 2019.\n",
       "> \n",
       "> [18] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Embedding transfer with label relaxation for improved metric learning. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3967–3976, June 2021.\n",
       "> \n",
       "> [19] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Self-taught metric learning without labels. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 7431–7441, 2022.\n",
       "> \n",
       "> [20] Sungwook Lee, Seunghyun Lee, and Byung Cheol Song. CFA: Coupled-hypersphere-based feature adaptation for target-oriented anomaly localization. *IEEE Access*, 10:78446–78454, 2022.\n",
       "> \n",
       "> [21] Jiarui Lei, Xiaobo Hu, Yue Wang, and Dong Liu. PyramidFlow: High-resolution defect contrastive localization using pyramid normalizing flow. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 14143–14152, 2023.\n",
       "> \n",
       "> [22] Christopher Liao, Theodoros Tsiligkaridis, and Brian Kulis. Supervised metric learning to rank for retrieval via contextual similarity optimization. *arXiv preprint arXiv:2210.01908*, 2022.\n",
       "> \n",
       "> [23] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. *arXiv preprint arXiv:1608.03983*, 2016.\n",
       "> \n",
       "> [24] TorchVision maintainers and contributors. TorchVision: PyTorch's computer vision library. https://github.com/pytorch/vision, 2016.\n",
       "> \n",
       "> [25] Leland McInnes, John Healy, and James Melville. UMAP: Uniform manifold approximation and projection for dimension reduction. *arXiv preprint arXiv:1802.03426*, 2018.\n",
       "> \n",
       "> [26] Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti. VT-ADL: A vision transformer network for image anomaly detection and localization. *2021 IEEE 30th International Symposium on Industrial Electronics (ISIE)*, pages 01–06. IEEE, 2021.\n",
       "> \n",
       "> [27] Duc Tam Nguyen, Zhongyu Lou, Michael Klar, and Thomas Brox. Anomaly detection with multiple-hypotheses predictions. *International Conference on Machine Learning*, pages 4800–4809. PMLR, 2019.\n",
       "> \n",
       "> [28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library, 2019.\n",
       "> \n",
       "> [29] Stanislav Pidhorskyi, Ranya Almohsen, and Gianfranco Doretto. Generative probabilistic novelty detection with adversarial autoencoders. *Advances in neural information processing systems*, 31, 2018.\n",
       "> \n",
       "> [30] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 14318–14328, 2022.\n",
       "> \n",
       "> [31] Marco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same same but different: Semi-supervised de...\n",
       "> \n",
       "> The text was missing the continuation of reference [31]. I have included the ellipsis (...) to indicate that the reference continues. You can add the missing part of the reference to complete the formatting. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## References\n",
       "> \n",
       "> **[31]** Marco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same same but differnet: Semi-supervised defect detection with normalizing flows. In *Proceedings of the IEEE/CVF winter conference on applications of computer vision*, pages 1907–1916, 2021.\n",
       "> \n",
       "> **[32]** Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Müller, and Marius Kloft. Deep one-class classification. In *International conference on machine learning*, pages 4393–4402. PMLR, 2018.\n",
       "> \n",
       "> **[33]** Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli. Adversarially learned one-class classifier for novelty detection. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 3379–3388, 2018.\n",
       "> \n",
       "> **[34]** Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimensionality reduction. In *Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis*, pages 4–11, 2014.\n",
       "> \n",
       "> **[35]** Adrian J Simpson and Mike J Fitter. What is the best index of detectability? *Psychological Bulletin*, 80(6):481, 1973.\n",
       "> \n",
       "> **[36]** Samarth Sinha, Han Zhang, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, and Augustus Odena. Small-GAN: Speeding up GAN training using core-sets. 119:9005–9015, 13–18 Jul 2020.\n",
       "> \n",
       "> **[37]** Tran Dinh Tien, Anh Tuan Nguyen, Nguyen Hoang Tran, Ta Duc Huy, Soan Duong, Chanh D Tr Nguyen, and Steven QH Truong. Revisiting reverse distillation for anomaly detection. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 24511–24520, 2023.\n",
       "> \n",
       "> **[38]** Guido Van Rossum and Fred L. Drake. *Python 3 Reference Manual*. CreateSpace, Scotts Valley, CA, 2009.\n",
       "> \n",
       "> **[39]** Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.\n",
       "> \n",
       "> **[40]** Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 1492–1500, 2017.\n",
       "> \n",
       "> **[41]** Minghui Yang, Peng Wu, and Hui Feng. Memseg: A semi-supervised method for image surface defect detection using differences and commonalities. *Engineering Applications of Artificial Intelligence*, 119:105835, 2023.\n",
       "> \n",
       "> **[42]** Jihun Yi and Sungroh Yoon. Patch svdd: Patch-level svdd for anomaly detection and segmentation. In *Proceedings of the Asian Conference on Computer Vision*, 2020.\n",
       "> \n",
       "> **[43]** Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, and Liwei Wu. Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. arXiv preprint arXiv:2111.07677, 2021.\n",
       "> \n",
       "> **[44]** Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\n",
       "> \n",
       "> **[45]** Vitjan Zavrtanik, Matej Kristan, and Danijel Skočaj. Draem-a discriminatively trained reconstruction embedding for surface anomaly detection. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 8330–8339, 2021.\n",
       "> \n",
       "> **[46]** Hui Zhang, Zuxuan Wu, Zheng Wang, Zhineng Chen, and Yu-Gang Jiang. Pro ... \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## ReConPatch: Contrastive Patch Representation Learning for Industrial Anomaly Detection\n",
       "> \n",
       "> **Supplementary:**\n",
       "> \n",
       "> ### Appendix A: Implementation Details\n",
       "> \n",
       "> ReConPatch was implemented using Python 3.7 [38] and PyTorch 1.9 [28]. Experiments were run on an Nvidia GeForce GTX 3090 GPU. ImageNet-pretrained models were utilized from torchvision [24] and the PyTorch Image Models repository [39]. Following [8] and [10], ReConPatch uses WideResNet50-backbone and WideResNet101-backbone [44] by default, for direct comparability. Patch-level features are extracted from the feature map aggregation of the final outputs in the *f* layer of ReConPatch. We use faiss [17] to perform all nearest neighbor retrieval and distance calculations, similar to PatchCore [30].\n",
       "> \n",
       "> ### Appendix B: Additional Experiments on MVTec AD\n",
       "> \n",
       "> This section details the setup of hyperparameters and experiments with the projection layer of ReConPatch on the MVTec AD [4] dataset. We experimented to find optimal hyperparameters for the following values: \n",
       "> \n",
       "> * **k:** The value of k-nearest neighborhood for calculating contextual similarity.\n",
       "> * **m:** The repelling margin in the RC loss.\n",
       "> * **α:** The ratio between pairwise and contextual similarity.\n",
       "> \n",
       "> Additionally, we tested our proposed method without the projection layer *g* to verify its effectiveness.\n",
       "> \n",
       "> **All experiments were conducted under the same conditions:**\n",
       "> \n",
       "> * Images were resized to 256 x 256 and center-cropped to 224 x 224.\n",
       "> * An ImageNet-pretrained WideResNet50-backbone [44] from torchvision [24] was used as the feature extractor.\n",
       "> * The *f* layer output size was set to 512.\n",
       "> * The coreset subsampling percentage was set to 1%.\n",
       "> * ReConPatch was trained for 120 epochs per class on MVTec AD [4] using the AdamP [15] optimizer with a cosine annealing [23] scheduler.\n",
       "> \n",
       "> #### B.1 The *k* Value of k-nearest Neighbor for Contextual Similarity\n",
       "> \n",
       "> We tested various *k*-nearest neighborhood values (*k*) to find the optimal performance for anomaly detection on the MVTec AD [4] dataset. In Eq. 2, smaller *k* values utilize features that are closer together to determine contextual similarity, while larger *k* values utilize features further apart in the embedding space. To find an optimal *k* value, we fixed other hyperparameters to *m* = 1 and *α* = 0.5. Table S1 shows the results of the experiments:\n",
       "> \n",
       "> | ReConPatch (WRS-50, 224 x 224, *α* = 0.5, *m* = 1.0) | *k* Value | Detection | Segmentation |\n",
       "> |---|---|---|---|\n",
       "> | | 3 | 99.5 | 98.09 |\n",
       "> | | 5 | 99.56 | 98.07 |\n",
       "> | | 7 | 99.55 | 98.07 |\n",
       "> | | 10 | 99.54 | 98.07 |\n",
       "> | | 15 | 99.5 | 98.04 |\n",
       "> | | 20 | 99.49 | - |\n",
       "> \n",
       "> **References:**\n",
       "> \n",
       "> [4] **MVTec AD**: https://www.mvtec.com/company/research/datasets/mvtec-ad\n",
       "> \n",
       "> [8] **[Reference 8]**\n",
       "> \n",
       "> [10] **[Reference 10]**\n",
       "> \n",
       "> [15] **AdamP**: https://arxiv.org/abs/1904.00962\n",
       "> \n",
       "> [17] **faiss**: https://faiss.ai/\n",
       "> \n",
       "> [23] **Cosine Annealing**: https://arxiv.org/abs/1608.03983\n",
       "> \n",
       "> [24] **torchvision**: https://pytorch.org/vision/stable/index.html\n",
       "> \n",
       "> [28] **PyTorch**: https://pytorch.org/\n",
       "> \n",
       "> [30] **PatchCore**: [Reference 30]\n",
       "> \n",
       "> [38] **Python 3.7**: https://www.python.org/\n",
       "> \n",
       "> [39] **PyTorch Image Models**: https://github.com/rwightman/pytorch-image-models\n",
       "> \n",
       "> [44] **WideResNet**: https://arxiv.org/abs/1605.07146 \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ##  Hyperparameter Analysis of ReConPatch for Anomaly Detection\n",
       "> \n",
       "> This document analyzes the influence of various hyperparameters on the performance of the ReConPatch model for anomaly detection, using the MVTec AD dataset.\n",
       "> \n",
       "> ### B.1  Impact of k-Nearest Neighborhood\n",
       "> \n",
       "> | k | Detection (AUROC) | Segmentation (AUROC) |\n",
       "> |---|---|---|\n",
       "> | 5 | 99.50 | 98.09 |\n",
       "> | 7 | 99.56 | 98.07 |\n",
       "> | 10 | 99.55 | 98.07 |\n",
       "> | 15 | 99.54 | 98.07 |\n",
       "> | 20 | 99.50 | 98.04 |\n",
       "> \n",
       "> **Table S1:** Results of anomaly detection (image-level AUROC) and segmentation (pixel-level AUROC) with various k-nearest neighborhood on the MVTec AD dataset. (α = 0.5, m = 1.0).\n",
       "> \n",
       "> ### B.2 Repelling Margin 'm' of Relaxed Contrastive Loss\n",
       "> \n",
       "> | m value | Detection (AUROC) | Segmentation (AUROC) |\n",
       "> |---|---|---|\n",
       "> | 0.5 | 99.55 | 98.04 |\n",
       "> | 1.0 | 99.56 | 98.07 |\n",
       "> | 1.5 | 99.55 | 98.05 |\n",
       "> | 2.0 | 99.50 | 98.04 |\n",
       "> \n",
       "> **Table S2:** Results of anomaly detection (image-level AUROC) and segmentation (pixel-level AUROC) with various repelling margin m values of RC loss. (k = 5, α = 0.5).\n",
       "> \n",
       "> The results show that changing the margin *m* has no significant effect on anomaly detection performance but slightly affects segmentation performance. We set *m* to 1.0 for subsequent experiments.\n",
       "> \n",
       "> ### B.3 Applying Ratio 'α' Between Pairwise and Contextual Similarity\n",
       "> \n",
       "> | α | Detection (AUROC) | Segmentation (AUROC) |\n",
       "> |---|---|---|\n",
       "> | 0.0 | 99.51 | 98.02 |\n",
       "> | 0.25 | 99.51 | 98.07 |\n",
       "> | 0.5 | 99.56 | 98.07 |\n",
       "> | 0.75 | 99.52 | 98.10 |\n",
       "> | 1.0 | 99.50 | 98.12 |\n",
       "> \n",
       "> **Table S3:** Results of anomaly detection (image-level AUROC) and segmentation (pixel-level AUROC) with various α values within range [0, 1] of ReConPatch. (k = 5, m = 1.0).\n",
       "> \n",
       "> The best detection performance is achieved when both pairwise and contextual similarity are combined with equal ratios (α = 0.5).\n",
       "> \n",
       "> ### B.4 The Projection Layer 'g'\n",
       "> \n",
       "> **Figure S1:** Visualization of patch features obtained by ReConPatch trained with and without the projection layer *g*. Features are mapped into a 2-dimensional space using UMAP for visualization.\n",
       "> \n",
       "> The projection layer *g* encourages features with similar positions to cluster together, leading to better anomaly detection performance.\n",
       "> \n",
       "> | Projection layer | Detection (AUROC) |\n",
       "> |---|---|\n",
       "> | w/o Proj. Layer | 99.42 |\n",
       "> | w/ Proj. Layer | 99.56 |\n",
       "> \n",
       "> **Table S4:** Results of Detection AUROC with and without the projection layer *g* on the MVTec AD dataset using ReConPatch with a WideResNet-50 backbone, 224 x 224 input size, 512 dimensional *f* layer, k = 5, m = 1.0, α = 1.0 and 1% coreset sampling.\n",
       "> \n",
       "> **Refer to caption:** Figure S1 illustrates the impact of the projection layer *g* on patch feature visualization.\n",
       "> \n",
       "> **Conclusion:** This analysis demonstrates the impact of various hyperparameters on the performance of ReConPatch.  The results highlight the importance of the projection layer *g* for effective anomaly detection.  Choosing appropriate values for k, m, and α optimizes the model's performance for specific use cases. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## Appendix C: Performance of ReConPatch\n",
       "> \n",
       "> ### C.1 Applying Data Augmentation\n",
       "> \n",
       "> In the industrial domain, product images are collected in a well-controlled environment. However, uncontrollable changes in environmental conditions during acquisition can cause variation in image data. This variation between normal image data can adversely affect the performance of industrial anomaly detection. To verify that ReConPatch is adaptable to environmental changes, we applied various data augmentation methods to evaluate its anomaly detection performance. \n",
       "> \n",
       "> We randomly applied rotation (±5 degrees), translation (±1%), color jitter (brightness and contrast ±30%), and Gaussian blur (σ = [0.1, 1.0]) which could occur in the real world. Table S5 shows the detailed performance comparison of PatchCore[30] and ReConPatch with data augmentation on the MVTec AD dataset[4].\n",
       "> \n",
       "> | Method | PatchCore | ReConPatch |\n",
       "> |---|---|---|\n",
       "> | Class/Aug. Method → | w/o Aug. | w/ Aug. | Diff | w/o Aug. | w/ Aug. | Diff |\n",
       "> | Bottle | 100 | 99.68 | 0.32 | 100 | 100 | 0 |\n",
       "> | Cable | 99.5 | 96.59 | 2.91 | 99.83 | 99.01 | 0.82 |\n",
       "> | Capsule | 98.1 | 82.73 | 15.37 | 98.8 | 95.29 | 3.51 |\n",
       "> | Hazelnut | 100 | 100 | 0 | 100 | 100 | 0 |\n",
       "> | Metal nut | 100 | 98.92 | 1.08 | 100 | 99.9 | 0.1 |\n",
       "> | Pill | 96.6 | 92.09 | 4.51 | 97.49 | 94.73 | 2.76 |\n",
       "> | Screw | 98.1 | 89.75 | 8.35 | 98.52 | 89.96 | 8.56 |\n",
       "> | Toothbrush | 100 | 95 | 5 | 100 | 99.17 | 0.83 |\n",
       "> | Transistor | 100 | 98.37 | 1.63 | 100 | 99.33 | 0.67 |\n",
       "> | Zipper | 99.4 | 95.51 | 3.89 | 99.76 | 99.13 | 0.63 |\n",
       "> | **Object classes** | **99.17** | **94.86** | **4.31** | **99.44** | **97.65** | **1.79** |\n",
       "> | Carpet | 98.7 | 96.47 | 2.23 | 99.6 | 99.04 | 0.56 |\n",
       "> | Grid | 98.2 | 87.89 | 10.31 | 100 | 98.5 | 1.5 |\n",
       "> | Leather | 100 | 100 | 0 | 100 | 100 | 0 |\n",
       "> | Tile | 98.7 | 96.64 | 2.06 | 99.78 | 100 | -0.22 |\n",
       "> | Wood | 99.2 | 99.47 | -0.27 | 99.65 | 99.82 | -0.17 |\n",
       "> | **Texture classes** | **98.96** | **96.09** | **2.87** | **99.81** | **99.47** | **0.34** |\n",
       "> | **Average** | **99.1** | **95.48** | **3.62** | **99.56** | **98.56** | **1.0** |\n",
       "> \n",
       "> **Table S5:** Anomaly detection performance with data augmentation (random rotate, translate, brightness, contrast, and Gaussian blur) on the MVTec AD dataset[4].\n",
       "> \n",
       "> ### C.2 Inference Time\n",
       "> \n",
       "> We measured the inference time and memory usage of ReConPatch and PatchCore[30] using an Intel Xeon Gold 6240 CPU and an Nvidia GeForce GTX 3090 GPU. Test images were resized to 256 × 256 and center-cropped to 224 × 224 on the MVTec AD dataset[4]. The target embed dimension was set to 512 in both PatchCore[30] and ReConPatch. We experimented for 30 iterations with 10 warm-up times to measure the inference time of PatchCore[30] and our proposed ReConPatch. In Table S6, the average inference time of PatchCore[30] and ReConPatch are not significantly different.\n",
       "> \n",
       "> | Method | Avg. inference time (msec) |\n",
       "> |---|---|\n",
       "> | PatchCore | 37.89 |\n",
       "> | ReConPatch | 38.09 |\n",
       "> \n",
       "> **Table S6:** Inference time comparison between PatchCore[30] and ReConPatch on the MVTec AD dataset.\n",
       "> \n",
       "> ### C.3 Memory Efficiency\n",
       "> \n",
       "> Our proposed ReConPatch algorithm uses a single linear layer ... \n",
       "> \n",
       "> **(Continue with the remaining text about memory efficiency)** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## Appendix C: ReConPatch Algorithm Details\n",
       "> \n",
       "> ### C.3 Memory Efficiency\n",
       "> \n",
       "> Our proposed ReConPatch algorithm utilizes a single linear layer trained with local patch features of normal samples. In Equation S1, the memory usage of the memory bank, 𝑀𝑐𝑜𝑟𝑒𝑠𝑒𝑡, is influenced by the dimension of the *f* layer, 𝑓𝑑𝑖𝑚, and the percentage of coreset subsampling, 𝑐𝑝𝑒𝑟𝑐𝑒𝑛𝑡𝑎𝑔𝑒. 𝐻𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠 and 𝑊𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠 represent the size of outputs in blocks 2 and 3, respectively. The results of anomaly detection and segmentation with various *f* layer dimensions are presented in Table 2 and Figure S2. In Figure S2, ReConPatch's performance for anomaly detection (image-level AUROC) and segmentation (pixel-level AUROC) surpasses PatchCore[30] with the same memory bank size.\n",
       "> \n",
       "> ```\n",
       "> 𝑀𝑐𝑜𝑟𝑒𝑠𝑒𝑡 = 𝑓𝑑𝑖𝑚 * 𝐻𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠 * 𝑊𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠 * 𝑁𝑡𝑟𝑎𝑖𝑛 * 𝑐𝑝𝑒𝑟𝑐𝑒𝑛𝑡𝑎𝑔𝑒 (S1) \n",
       "> ```\n",
       "> \n",
       "> **Refer to caption**\n",
       "> \n",
       "> **Figure S2:** The results of Detection AUROC and Segmentation AUROC for the *f* layer dimension on the MVTec AD dataset[4]. Image size is 224 × 224, *k* = 5, *m* = 1.0, *α* = 1.0, and coreset subsampling rate is 1%.\n",
       "> \n",
       "> ### C.4 Anomaly Score Maps\n",
       "> \n",
       "> We provide examples of the anomaly score map for the MVTec AD dataset[4] and BTAD[26] dataset alongside ground truth (orange line) overlaid on input images in Figure S3 and S4. The anomaly map indicates regions in the input image where ReConPatch has detected anomalies, with higher values indicating a higher likelihood of an anomaly. The green line represents the threshold optimized by the F1 scores of anomaly segmentation, and the orange line represents the ground truth of the anomalies.\n",
       "> \n",
       "> **Refer to caption**\n",
       "> \n",
       "> **Figure S3:** Examples of images with anomalies (top) and measured anomaly score maps (bottom) on the MVTec AD dataset[4]. The orange line depicts the ground truth of the anomalies, and the green line depicts thresholds optimizing F1 scores of anomaly segmentation. The green star indicates the maximal location of the anomaly score in the heatmap.\n",
       "> \n",
       "> **Refer to caption**\n",
       "> \n",
       "> **Figure S4:** Examples of images with anomalies (top) and measured anomaly score maps (bottom) on the BTAD dataset[26]. The orange line depicts the ground truth of the anomalies, and the green line depicts thresholds optimizing F1 scores of anomaly segmentation. The green star indicates the maximal location of the anomaly score in the heatmap.\n",
       "> \n",
       "> ## Appendix D: Refinement Network for Fine-grained Anomaly Localization\n",
       "> \n",
       "> This section details the improved performance achieved through the combination of ReConPatch and the refinement network. Since ReConPatch's patch-level score map is obtained by aggregating features from intermediate layers, it requires resolution matching between the score map and the input image. To match the resolution, ReConPatch utilizes bilinear upsampling and Gaussian smoothing. However, this approach fails to represent abnormal regions with the desired level of detail. To address this issue, ... \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## Refining Anomaly Detection with a Refinement Network\n",
       "> \n",
       "> This document discusses the implementation of a refinement network to improve the performance of anomaly detection models, specifically focusing on the ReConPatch model in the MVTec AD dataset.\n",
       "> \n",
       "> ### Motivation\n",
       "> \n",
       "> The existing approach using ReConPatch fails to accurately represent abnormal regions with sufficient detail. To address this limitation, a refinement network is introduced, inspired by previous work [2, 45, 41, 46].\n",
       "> \n",
       "> ### Refinement Network Design\n",
       "> \n",
       "> The refinement network consists of:\n",
       "> \n",
       "> * Two residual blocks.\n",
       "> * Parallel Atrous convolutions with varying rates (Atrous Spatial Pyramid Pooling [47]).\n",
       "> \n",
       "> The network is trained using:\n",
       "> \n",
       "> * **SGD optimizer** for 500 epochs.\n",
       "> * **Batch size:** 16\n",
       "> * **Learning rate:** 10<sup>-2</sup>\n",
       "> * **Momentum:** 0.9\n",
       "> * **Weight decay:** 10<sup>-4</sup>\n",
       "> \n",
       "> For comparison with the ensemble ReConPatch model, each image is resized and center-cropped to the size specified in Section 4.3.\n",
       "> \n",
       "> ### Abnormal Image Simulation\n",
       "> \n",
       "> To train the refinement network, synthetic abnormal images are generated, as real abnormal images are unavailable for this task. Building upon previous work [45, 41, 46], synthetic images are created by:\n",
       "> \n",
       "> 1. **Mixing:** Randomly sampled masks of normal images with images from an external dataset [7]. This simulates textural anomalies.\n",
       "> 2. **Permuting:** Randomly permuting image patches, as proposed in MemSeg [41]. This simulates structural anomalies.\n",
       "> \n",
       "> **Performance Comparison:**\n",
       "> \n",
       "> Table S7 shows that using structural anomalies yields the best performance across MVTec AD classes. Analyzing anomaly score maps reveals that structural deviations exhibit smaller anomaly score variations compared to textural anomalies. This smaller deviation contributes to improved discrimination based on image-level anomaly scores.\n",
       "> \n",
       "> **Table S7: Evaluating Abnormal Simulation Strategies on MVTec AD Benchmark**\n",
       "> \n",
       "> | Method                           | w/o Refinement | w/ Refinement [47] |\n",
       "> |-----------------------------------|------------------|----------------------|\n",
       "> | Simulation type                 | -               | Texture [45]        | Structure [41]       |\n",
       "> | Detection                        | 99.56           | 98.91               | 99.71               |\n",
       "> | Segmentation                     | 98.18           | 98.43               | 98.62               |\n",
       "> \n",
       "> ### Anomaly Detection and Localization on MVTec AD\n",
       "> \n",
       "> Table S8 demonstrates the performance improvement of ReConPatch equipped with the refinement network. The results are averaged across all MVTec AD classes.\n",
       "> \n",
       "> **Table S8: Refinement Network Performance on MVTec AD**\n",
       "> \n",
       "> | Method                            | AUROC (Image-level) | AUROC (Pixel-level) |\n",
       "> |-------------------------------------|---------------------|-----------------------|\n",
       "> | ReConPatch (WRS-50, 224x224)      | -                   | -                     |\n",
       "> | ReConPatch + Refinement [47]      | Improved            | Improved              |\n",
       "> \n",
       "> **Conclusion:**\n",
       "> \n",
       "> The refined anomaly score maps significantly improve both image-level and pixel-level performance, showcasing the effectiveness of the refinement network in enhancing anomaly detection capabilities.\n",
       "> \n",
       "> ### Ensem\n",
       "> \n",
       "> This section appears incomplete and requires further context. \n",
       "> \n",
       "> **Please provide additional details about the 'Ensem' section for a more complete understanding of the content.** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> ## Ensemble Backbone Performance on MVTec AD Dataset\n",
       "> \n",
       "> **Table S8:** Comparison of ensemble model anomaly detection (image-level AUROC) and segmentation (pixel-level AUROC) performance on the MVTec AD dataset.\n",
       "> \n",
       "> | Ensemble Backbone | WRN-101 & RNext-101 & DenseN-201 | WRN-101 & RNext-101 & DenseN-201 |\n",
       "> |---|---|---|---|---|\n",
       "> | Image Size | 480 × 480 | 320 × 320 | 480 × 480 | 320 × 320 |\n",
       "> | Method | ReConPatch | ReConPatch (w/ refine) | ReConPatch | ReConPatch (w/ refine) |\n",
       "> | Detection (AUROC) | 99.72 | 99.86 | 99.67 | 99.78 |\n",
       "> | Segmentation (AUROC) | 98.67 | 99.20 | 98.36 | 98.96 |\n",
       "> \n",
       "> **Figure S5:** Visualization of the anomaly score map comparing before and after refinement.\n",
       "> \n",
       "> * **Second row:** Anomaly score map of ReConPatch without refinement network.\n",
       "> * **Third row:** Anomaly score map of ReConPatch with refinement network. Refined score maps show more fine-grained localization of defects in the images.\n",
       "> * **Bottom row:** Pixel-wise ROC curve for each anomaly score map. The refined score map surpasses the score map without refinement.\n",
       "> \n",
       "> **Performance improvement:** The refined anomaly score maps achieve better performance in both image-level and pixel-level AUROC. This improvement arises from reducing the anomaly scores on irrelevant locations, such as the background, leading to more precise defect localization.\n",
       "> \n",
       "> **Refer to:** Caption of Figure S5. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display, Markdown\n",
    "from google.api_core import retry\n",
    "import concurrent.futures\n",
    "\n",
    "def to_markdown(text):\n",
    "    text = text.replace('•', '  *')\n",
    "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "def load_api_key(filepath):\n",
    "    return pathlib.Path(filepath).read_text().strip()\n",
    "\n",
    "def configure_genai(api_key):\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "def get_model(model_name):\n",
    "    generation_config = {\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 64,\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"response_mime_type\": \"text/plain\",\n",
    "        }\n",
    "    \n",
    "    return genai.GenerativeModel(model_name=model_name, generation_config=generation_config)\n",
    "\n",
    "def generate_markdown(model, context):\n",
    "    prompt = f\"\"\"\n",
    "    Please convert the following text into a well-formatted Markdown document:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt, request_options={'retry': retry.Retry()})\n",
    "    return response\n",
    "\n",
    "def process_response(response):\n",
    "    try:\n",
    "        markdown_text = response.candidates[0].content.parts[0].text\n",
    "        return markdown_text\n",
    "    except KeyError:\n",
    "        return \"\"\n",
    "\n",
    "def display_markdown(markdown_text):\n",
    "    display(to_markdown(markdown_text))\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap_size=100):\n",
    "    \"\"\"텍스트를 청크로 나누되, 각 청크 사이에 overlap_size만큼 겹치게 나눕니다.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap_size  # 겹치는 부분을 포함해서 다음 청크 시작점 설정\n",
    "    return chunks\n",
    "\n",
    "def main(document, api_key_filepath, model_name, chunk_size=500, overlap_size=100, max_concurrent_requests=8):\n",
    "    # API 키 로드 및 설정\n",
    "    api_key = load_api_key(api_key_filepath)\n",
    "    configure_genai(api_key)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = get_model(model_name)\n",
    "\n",
    "    # 문서 청크로 나누기\n",
    "    chunks = chunk_text(document, chunk_size, overlap_size)\n",
    "\n",
    "    print(len(document))\n",
    "    print(len(chunks))\n",
    "    # 병렬 API 호출 수행\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_requests) as executor:\n",
    "        futures = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            futures.append((i, executor.submit(generate_markdown, model, chunk)))\n",
    "            if (i + 1) % max_concurrent_requests == 0:\n",
    "                for index, future in futures:\n",
    "                    response = future.result()\n",
    "                    markdown_text = process_response(response)\n",
    "                    chunks[index] = markdown_text\n",
    "                futures = []\n",
    "\n",
    "        # 남은 청크 처리\n",
    "        for index, future in futures:\n",
    "            response = future.result()\n",
    "            markdown_text = process_response(response)\n",
    "            chunks[index] = markdown_text\n",
    "\n",
    "    # 순서대로 결과 출력\n",
    "    for chunk in chunks:\n",
    "        display_markdown(chunk)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    document = \"\"\"\n",
    "License: arXiv.org perpetual non-exclusive license\n",
    "arXiv:2305.16713v3 [cs.CV] 10 Jan 2024\n",
    "ReConPatch : Contrastive Patch Representation Learning for Industrial Anomaly Detection\n",
    "Jeeho Hyun   Sangyun Kim   Giyoung Jeon   Seung Hwan Kim\n",
    "Kyunghoon Bae   Byung Jun Kang\n",
    "LG AI Research\n",
    "Correspondence to: bj.kang@lgresearch.ai\n",
    "Abstract\n",
    "Anomaly detection is crucial to the advanced identification of product defects such as incorrect parts, misaligned components, and damages in industrial manufacturing. Due to the rare observations and unknown types of defects, anomaly detection is considered to be challenging in machine learning. To overcome this difficulty, recent approaches utilize the common visual representations pre-trained from natural image datasets and distill the relevant features. However, existing approaches still have the discrepancy between the pre-trained feature and the target data, or require the input augmentation which should be carefully designed, particularly for the industrial dataset. In this paper, we introduce ReConPatch, which constructs discriminative features for anomaly detection by training a linear modulation of patch features extracted from the pre-trained model. ReConPatch employs contrastive representation learning to collect and distribute features in a way that produces a target-oriented and easily separable representation. To address the absence of labeled pairs for the contrastive learning, we utilize two similarity measures between data representations, pairwise and contextual similarities, as pseudo-labels. Our method achieves the state-of-the-art anomaly detection performance (99.72%) for the widely used and challenging MVTec AD dataset. Additionally, we achieved a state-of-the-art anomaly detection performance (95.8%) for the BTAD dataset.\n",
    "\n",
    "1Introduction\n",
    "Anomaly detection in industrial manufacturing is key to identify the defects in products and maintain their quality. Anomalies can include incorrect parts, misaligned components, or damage to the product. Machine learning approaches for anomaly detection have been widely studied with an increasing demand for the automation in industrial applications. The main concern of these approaches is to learn how to discriminate anomalies from normal cases based on previously collected data. However, anomaly detection is particularly challenging because defects are rarely observed and unknown types of defects can occur. Such situation, in which the majority of cases are marked as normal and abnormal cases are scarce in the collected data, has lead to the improvements in one-class classification.\n",
    "\n",
    "The key concept of one-class classification for anomaly detection is to train a model to learn a distance metric between data and detect anomalies at a large distance from the nominal data. In an effort to learn the metric, reconstruction-based approaches have been proposed to detect anomalies by measuring the reconstruction errors using auto-encoding models [9, 27, 34] or generative adversarial networks (GANs) [29, 33]. As the variety of data is not sufficiently rich to estimate a reliable nominal distribution from scratch, recent works have shown that leveraging the common visual representation, obtained from a natural image dataset [12], can result in high anomaly detection performance [3, 8]. Although pre-trained models can provide rich representations without adaptation, such representations are not sufficiently distinguishable to identify subtle defects in industrial images. The distribution shift between natural and industrial images also makes it difficult to extract anomaly-specific features. For improvements in anomaly detection performance, it is essential to train a model to learn a representation space that effectively discriminates borderline anomalies.\n",
    "\n",
    "To alleviate the distribution shift between the pre-trained and the industrial datasets, prominent features for anomaly detection can be distilled by training a student model to reproduce the representation of the pre-trained model using a teacher supervision [5]. Attaching a normalizing flow [13] at the end of the pre-trained model is another approach to exploit the pre-trained representation and estimate the distribution of normality [31]. Unfortunately, existing methods still require extensive handcrafted input augmentation, such as random crop, random rotation, or color jitter. Particularly in case of industrial images, data augmentation should be carefully designed by the user expertise.\n",
    "\n",
    "In this paper, we introduce unsupervised metric learning framework for anomaly detection by enhancing the arrangement of the features, ReConPatch. Contrastive learning-based training schemes present weaknesses in terms of modeling variations within nominal instances, which may increase the false-positive rate of the anomaly detection. To this end, ReConPatch utilizes the contextual similarity [19] among features obtained from the model as a pseudo-label for the training. Specifically, our method efficiently adapts feature representation by training only a simple linear transformation, as opposed to training the entire network. By doing so, we are able to learn a target-oriented feature representation which achieves higher anomaly detection accuracy without input augmentation, making our method a practical and effective solution for anomaly detection in various industrial settings.\n",
    "\n",
    "Refer to caption\n",
    "Figure 1:Overall structure of the anomaly detection using ReConPatch. ReConPatch consists of two networks to train representations of the patch-level features, which includes the feature representation layer \n",
    "𝑓\n",
    ", \n",
    "𝑓\n",
    "¯\n",
    " and projection layer \n",
    "𝑔\n",
    ", \n",
    "𝑔\n",
    "¯\n",
    " respectively. Upper networks (\n",
    "𝑓\n",
    "¯\n",
    ",\n",
    "𝑔\n",
    "¯\n",
    ") are used to calculate pairwise and contextual similarities between patch-level feature pairs, while the bottom networks (\n",
    "𝑓\n",
    ",\n",
    "𝑔\n",
    ") used for the representation learning of patch-level features is trained through relaxed contrastive loss \n",
    "ℒ\n",
    "𝑅\n",
    "⁢\n",
    "𝐶\n",
    ".\n",
    "2Related Work\n",
    "Unsupervised machine learning approaches in anomaly detection using neural networks have been widely analyzed. Deep Support Vector Data Description (SVDD) trains a neural network to map each datum to the hyperspherical embedding and detect anomalies by measuring the distance from the center of the hypersphere [32]. Patch SVDD has been developed as a patch-wise extension of Deep SVDD, utilizing the features of each spatial patch from the convolutional neural network (CNN) feature map to enhance localization and enable fine-grained examination [42]. The reconstruction-based approach assumes that normal data can be accurately reconstructed or generated by training a model using a nominal dataset, whereas abnormal data cannot. Based on this assumption, an anomaly score is calculated as the error between the original input and the reconstructed input. Auto-encoding models are used for the reconstruction model [9, 27, 34]. With the improvements in GANs, several approaches have also shown the effectiveness of GANs in anomaly detection [29, 33]. When training a model from scratch, variety and abundance should be guaranteed, which is mostly not available for anomaly detection.\n",
    "\n",
    "To alleviate the shortage of data in anomaly detection, several attempts have been made to utilize a common visual representation pre-trained with a rich natural image dataset [12]. Previous studies that use such pre-trained model measures the distance between the representations of input data and their nearest neighbors to detect the anomalies [3] and compares hierarchical sub-image features to localize anomalies [8]. To efficiently compare the input with training set, a memory bank is introduced to store the representatives [8].\n",
    "\n",
    "DifferNet [31] provides a normalizing flow [13] that is helpful in training a bijective mapping between the pre-trained feature distribution and the well-defined density of the nominal data, which is used to identify the anomalies. A condition normalizing flow using positional encoding is proposed by CFLOW-AD [14]. As the normalizing flow is trained to map features to the nominal distribution, this method is vulnerable to the outliers in the training dataset.\n",
    "\n",
    "PatchCore proposes a locally aware patch feature and efficient greedy subsampling method to define the coreset [30]. The coupled-hypersphere-based feature adaptation (CFA) trains a patch descriptor that maps features onto the hypersphere, which is centered on the nearest neighbor in the memory bank [20]. PaDiM estimates a Gaussian distribution of patch features at each spatial location to detect and localize out-of-distributions (OODs) as anomalies [10]. PNI is developed to train a neural network to predict the feature distribution of each spatial location and its neighborhoods [2].\n",
    "\n",
    "3Method\n",
    "Our proposed method, ReConPatch, focuses on learning a representation space that maps features extracted from nominal image patches to be grouped closely if they share similar nominal characteristics in an unsupervised learning manner. Although previous work [30] has shown the effectiveness of selecting representative nominal patch features using a pre-trained model, this model still presents a representation biased to the natural image data, which has a gap with the target data. The main concept of our proposed approach is to train the target-oriented features that spread out the distributions of patch features according to the variations in normal samples, and gathers the similar features.\n",
    "\n",
    "3.1Overall structure\n",
    "As shown in Fig. 1, our framework consists of the training and the inference phases. In the training phase, we first collect the feature map at layer \n",
    "𝑙\n",
    ", \n",
    "Φ\n",
    "𝑙\n",
    "⁢\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "∈\n",
    "ℝ\n",
    "𝐶\n",
    "×\n",
    "𝐻\n",
    "×\n",
    "𝑊\n",
    " for each input \n",
    "𝑥\n",
    " in the training data using the pre-trained CNN model.\n",
    "\n",
    "The feature maps have different spatial resolutions at the feature hierarchy of the CNN, so they are interpolated to have the same resolution before being concatenated. Patch-level features \n",
    "𝒫\n",
    "⁢\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "ℎ\n",
    ",\n",
    "𝑤\n",
    ")\n",
    "∈\n",
    "ℝ\n",
    "𝐶\n",
    "′\n",
    "1 then generated by aggregating the feature vectors of the neighborhood within a specific patch size \n",
    "𝑠\n",
    " in the same approach employed in PatchCore [30]. Adaptive average pooling is used for the local aggregation.\n",
    "\n",
    "ReConPatch utilizes two networks to train representations of the patch-level features. One of these is a network for patch-level feature representation learning, which is trained using the relaxed contrastive loss \n",
    "ℒ\n",
    "𝑅\n",
    "⁢\n",
    "𝐶\n",
    " in Eq. 7. The representation network is composed of a feature representation layer \n",
    "𝑓\n",
    " and the projection layer \n",
    "𝑔\n",
    " respectively. When computing the \n",
    "ℒ\n",
    "𝑅\n",
    "⁢\n",
    "𝐶\n",
    ", pseudo-labels should be provided for every pair of features. The other network is used to calculate pairwise and contextual similarities between patch-level feature pairs. In addition, the similarity calculation network is gradually updated by an exponential moving average (EMA) of the representation network. To distinguish the two networks, the layers in the latter network is denoted as \n",
    "𝑓\n",
    "¯\n",
    " and \n",
    "𝑔\n",
    "¯\n",
    " respectively.\n",
    "\n",
    "After training the representation, the patch-level features extracted from the pre-trained CNN are transformed into target-oriented features using the feature representation layer \n",
    "𝑓\n",
    " [6]. The representative features are selected using the coreset subsampling approach based on the greedy approximation algorithm [36] and stored in a memory bank. In the inference phase, the features of a test sample are extracted using the same process as training, and the anomaly score is calculated by comparing the features with the nominal representative in the memory bank.\n",
    "\n",
    "3.2Patch-level feature representation learning\n",
    "The objective of ReConPatch is to learn target-oriented features from patch-level features, thereby enabling more effective discrimination between normal and abnormal features. To accomplish this goal, a patch-level features representation learning approach is applied to aggregate highly similar features while repelling those with low similarity. However, such training requires labeled pairs to indicate the degree of proximity between patch-level features. To address this issue, we utilize the similarity between patch-level features using the pairwise similarity and the contextual similarities as pseudo-labels. The similarity is high, then the pair is pseudo-labeled as positive and vice versa.\n",
    "\n",
    "For two arbitrary patch-level features \n",
    "𝑝\n",
    "𝑖\n",
    " and \n",
    "𝑝\n",
    "𝑗\n",
    " obtained by \n",
    "𝒫\n",
    "⁢\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "ℎ\n",
    ",\n",
    "𝑤\n",
    ")\n",
    ", let the projected representation be \n",
    "𝑧\n",
    "¯\n",
    "𝑖\n",
    "=\n",
    "𝑔\n",
    "¯\n",
    "⁢\n",
    "(\n",
    "𝑓\n",
    "¯\n",
    "⁢\n",
    "(\n",
    "𝑝\n",
    "𝑖\n",
    ")\n",
    ")\n",
    " and \n",
    "𝑧\n",
    "¯\n",
    "𝑗\n",
    "=\n",
    "𝑔\n",
    "¯\n",
    "⁢\n",
    "(\n",
    "𝑓\n",
    "¯\n",
    "⁢\n",
    "(\n",
    "𝑝\n",
    "𝑗\n",
    ")\n",
    ")\n",
    ". The pairwise similarity between two features, \n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝑃\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑤\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑠\n",
    "⁢\n",
    "𝑒\n",
    ", is then provided by\n",
    "\n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝑃\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑤\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑠\n",
    "⁢\n",
    "𝑒\n",
    "=\n",
    "𝑒\n",
    "−\n",
    "‖\n",
    "𝑧\n",
    "¯\n",
    "𝑖\n",
    "−\n",
    "𝑧\n",
    "¯\n",
    "𝑗\n",
    "‖\n",
    "2\n",
    "2\n",
    "/\n",
    "𝜎\n",
    "(1)\n",
    "where \n",
    "𝜎\n",
    " is the bandwidth of the Gaussian kernel, which can be adjusted to tune the degree of smoothing in the similarity measure [18, 19].\n",
    "\n",
    "Refer to caption\n",
    "(a)\n",
    "Refer to caption\n",
    "(b)\n",
    "Figure 2:Illustrative examples of similarity measures in the representation space. The pairwise similarity \n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝑃\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑤\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑠\n",
    "⁢\n",
    "𝑒\n",
    " between \n",
    "𝑧\n",
    "¯\n",
    "𝑖\n",
    " and \n",
    "𝑧\n",
    "¯\n",
    "𝑗\n",
    " is identical in both (a) and (b). In (a), the \n",
    "𝑘\n",
    "-nearest neighbors \n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑖\n",
    ")\n",
    " and \n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑗\n",
    ")\n",
    " do not enclose each other. Therefore, \n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    " has a lower value, and the \n",
    "𝑧\n",
    "¯\n",
    "𝑖\n",
    " and \n",
    "𝑧\n",
    "¯\n",
    "𝑗\n",
    " pair should become apart. By contrast, as \n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑖\n",
    ")\n",
    " and \n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑗\n",
    ")\n",
    " enclose each other in (b) case, \n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    " takes a higher value, so that \n",
    "𝑧\n",
    "¯\n",
    "𝑖\n",
    " and \n",
    "𝑧\n",
    "¯\n",
    "𝑗\n",
    " pair should attract each other.\n",
    "We note that Eq. 1 is used to measure the Gaussian kernel similarity between \n",
    "𝑝\n",
    "𝑖\n",
    " and \n",
    "𝑝\n",
    "𝑗\n",
    ", which is widely used to measure anomaly scores. However, the pairwise similarity is insufficient to consider the relationships among groups of features. As depicted in Fig. 2, for example, cases (a) and (b) have the same pairwise similarity. In (a) case, \n",
    "𝑧\n",
    "¯\n",
    "𝑖\n",
    " and \n",
    "𝑧\n",
    "¯\n",
    "𝑗\n",
    " belong to different groups of features; therefore, they should be separated. By contrast, in (b), they belong to the same group and should be gathered.\n",
    "\n",
    "This leads to the simultaneous measure of contextual similarity, which consider the neighborhood of an embedding vector. Let \n",
    "𝑘\n",
    "-nearest neighborhood of the feature index \n",
    "𝑖\n",
    " is given as a set of indices, \n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "=\n",
    "{\n",
    "𝑗\n",
    "|\n",
    "𝑑\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "≤\n",
    "𝑑\n",
    "𝑖\n",
    "⁢\n",
    "𝑙\n",
    "}\n",
    " where \n",
    "𝑙\n",
    " is \n",
    "𝑘\n",
    "-th nearest neighbor and \n",
    "𝑑\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    " denotes the Euclidean distance between the two embedding vectors \n",
    "(\n",
    "𝑑\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "=\n",
    "∥\n",
    "𝑧\n",
    "¯\n",
    "𝑖\n",
    "−\n",
    "𝑧\n",
    "¯\n",
    "𝑗\n",
    "∥\n",
    "2\n",
    "2\n",
    ")\n",
    ". Two patch-level features can be regarded as contextually similar if they share more nearest neighbors in common [22]. The contextual similarity \n",
    "𝜔\n",
    "~\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    " between two patch-level features \n",
    "𝑝\n",
    "𝑖\n",
    " and \n",
    "𝑝\n",
    "𝑗\n",
    " is then defined as\n",
    "\n",
    "𝜔\n",
    "~\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    "=\n",
    "{\n",
    "|\n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "∩\n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑗\n",
    ")\n",
    "|\n",
    "|\n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "|\n",
    ",\n",
    "if \n",
    "⁢\n",
    "𝑗\n",
    "∈\n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "0\n",
    ",\n",
    "𝑜\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "ℎ\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑤\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑠\n",
    "⁢\n",
    "𝑒\n",
    ".\n",
    "(2)\n",
    "In addition, the approach developed in this study adopts the idea of query expansion, which is widely used to improve the information retrieval, by expanding the query to the neighbors of neighbors [19, 22]. \n",
    "𝜔\n",
    "~\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    " is redefined by averaging the similarities over the set of \n",
    "𝑘\n",
    "-nearest reciprocal neighbors.\n",
    "\n",
    "ℛ\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "=\n",
    "{\n",
    "𝑗\n",
    "|\n",
    "𝑗\n",
    "∈\n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "⁢\n",
    " and \n",
    "⁢\n",
    "𝑖\n",
    "∈\n",
    "𝒩\n",
    "𝑘\n",
    "⁢\n",
    "(\n",
    "𝑗\n",
    ")\n",
    "}\n",
    "(3)\n",
    "𝜔\n",
    "^\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    "=\n",
    "1\n",
    "|\n",
    "ℛ\n",
    "𝑘\n",
    "2\n",
    "⁢\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "|\n",
    "⁢\n",
    "∑\n",
    "𝑙\n",
    "∈\n",
    "ℛ\n",
    "𝑘\n",
    "2\n",
    "⁢\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "𝜔\n",
    "~\n",
    "𝑙\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    ".\n",
    "(4)\n",
    "Because \n",
    "𝑤\n",
    "^\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    " is asymmetric, the contextual similarity is finally defined as the average bi-directional similarity of a pair, which is given by\n",
    "\n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    "=\n",
    "1\n",
    "2\n",
    "⁢\n",
    "(\n",
    "𝜔\n",
    "^\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    "+\n",
    "𝜔\n",
    "^\n",
    "𝑗\n",
    "⁢\n",
    "𝑖\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    ")\n",
    ".\n",
    "(5)\n",
    "The final similarity between two patch-level features \n",
    "𝑝\n",
    "𝑖\n",
    " and \n",
    "𝑝\n",
    "𝑗\n",
    " is then defined as a linear combination of two similarities with a quantity \n",
    "𝛼\n",
    "∈\n",
    "[\n",
    "0\n",
    ",\n",
    "1\n",
    "]\n",
    ",\n",
    "\n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "=\n",
    "𝛼\n",
    "⋅\n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝑃\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑤\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑠\n",
    "⁢\n",
    "𝑒\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "𝛼\n",
    ")\n",
    "⋅\n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝐶\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑥\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    ".\n",
    "(6)\n",
    "Patch-level features do not have explicit labels because each patch image is correlated with neighboring patches. Moreover, the goal is to obtain unique target-oriented features rather than clearly distinguishing them. Thus, relaxed contrastive loss [18] was adopted, in which inter-feature similarity is considered as pseudo-labels. Let \n",
    "𝛿\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "=\n",
    "‖\n",
    "𝑧\n",
    "𝑖\n",
    "−\n",
    "𝑧\n",
    "𝑗\n",
    "‖\n",
    "2\n",
    "/\n",
    "(\n",
    "1\n",
    "𝑁\n",
    "⁢\n",
    "Σ\n",
    "𝑛\n",
    "=\n",
    "1\n",
    "𝑁\n",
    "⁢\n",
    "‖\n",
    "𝑧\n",
    "𝑖\n",
    "−\n",
    "𝑧\n",
    "𝑛\n",
    "‖\n",
    "2\n",
    ")\n",
    " denote the relative distance between embedding vectors in a mini-batch. The relaxed contrastive loss is given by\n",
    "\n",
    "ℒ\n",
    "𝑅\n",
    "⁢\n",
    "𝐶\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "=\n",
    "1\n",
    "𝑁\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑁\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑁\n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "𝛿\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    "2\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    ")\n",
    "max\n",
    "(\n",
    "𝑚\n",
    "−\n",
    "𝛿\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    ",\n",
    "0\n",
    ")\n",
    "2\n",
    "(7)\n",
    "where \n",
    "𝑧\n",
    " is the embedding vectors inferred by \n",
    "𝑔\n",
    "⁢\n",
    "(\n",
    "𝑓\n",
    "⁢\n",
    "(\n",
    "𝑝\n",
    ")\n",
    ")\n",
    ", \n",
    "𝑁\n",
    " is the number of instances in a mini-batch, and \n",
    "𝑚\n",
    " is the repelling margin. \n",
    "𝜔\n",
    "𝑖\n",
    "⁢\n",
    "𝑗\n",
    " in Eq. 7 determines the weights of the attracting and repelling loss terms.\n",
    "\n",
    "While representation learning networks \n",
    "𝑓\n",
    " and \n",
    "𝑔\n",
    " are trained with relaxed contrastive loss, the similarity calculation network \n",
    "𝑓\n",
    "¯\n",
    " and \n",
    "𝑔\n",
    "¯\n",
    " are slowly updated with an the EMA of the parameters in \n",
    "𝑓\n",
    " and \n",
    "𝑔\n",
    " respectively. Fast training of the similarity calculation network reduces the consistency of the relationships between the patch-level features, leading to unstable training. Let \n",
    "𝜃\n",
    "𝑓\n",
    "¯\n",
    ",\n",
    "𝑔\n",
    "¯\n",
    " be the parameters of the similarity calculation network and \n",
    "𝜃\n",
    "𝑓\n",
    ",\n",
    "𝑔\n",
    " be the parameters of the representation learning network. \n",
    "𝜃\n",
    "𝑓\n",
    "¯\n",
    ",\n",
    "𝑔\n",
    "¯\n",
    " is then updated by\n",
    "\n",
    "𝜃\n",
    "𝑓\n",
    "¯\n",
    ",\n",
    "𝑔\n",
    "¯\n",
    "←\n",
    "𝛾\n",
    "⋅\n",
    "𝜃\n",
    "𝑓\n",
    "¯\n",
    ",\n",
    "𝑔\n",
    "¯\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "𝛾\n",
    ")\n",
    "⋅\n",
    "𝜃\n",
    "𝑓\n",
    ",\n",
    "𝑔\n",
    "(8)\n",
    "where \n",
    "𝛾\n",
    " is the hyper-parameter that adjusts the rate of momentum update.\n",
    "\n",
    "3.3Anomaly detection with ReConPatch\n",
    "Anomaly scores are calculated in the same manner as in the case of PatchCore [30]. After training, the coreset is subsampled from the newly trained feature representation \n",
    "𝑓\n",
    "⁢\n",
    "(\n",
    "⋅\n",
    ")\n",
    " using the greedy approximation algorithm [36] and stored in memory bank \n",
    "ℳ\n",
    ". The coreset takes a role of the representative feature, which is used to compute the anomaly score. The pixel-wise anomaly score is then obtained by calculating the distance between the representation layer output, \n",
    "𝑓\n",
    "⁢\n",
    "(\n",
    "𝑝\n",
    "𝑡\n",
    ")\n",
    ", and its nearest coreset \n",
    "𝑟\n",
    "*\n",
    " within the memory bank,\n",
    "\n",
    "𝑟\n",
    "*\n",
    "=\n",
    "argmin\n",
    "𝑟\n",
    "∈\n",
    "ℳ\n",
    "𝒟\n",
    "⁢\n",
    "(\n",
    "𝑓\n",
    "⁢\n",
    "(\n",
    "𝑝\n",
    "𝑡\n",
    ")\n",
    ",\n",
    "𝑟\n",
    ")\n",
    ",\n",
    "(9)\n",
    "𝑠\n",
    "𝑡\n",
    "=\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑒\n",
    "𝑠\n",
    "𝑡\n",
    "′\n",
    "∑\n",
    "𝑟\n",
    "′\n",
    "∈\n",
    "𝒩\n",
    "𝑏\n",
    "⁢\n",
    "(\n",
    "𝑟\n",
    "*\n",
    ")\n",
    "𝑒\n",
    "𝒟\n",
    "⁢\n",
    "(\n",
    "𝑓\n",
    "⁢\n",
    "(\n",
    "𝑝\n",
    "𝑡\n",
    ")\n",
    ",\n",
    "𝑟\n",
    "′\n",
    ")\n",
    ")\n",
    "⁢\n",
    "𝒟\n",
    "⁢\n",
    "(\n",
    "𝑓\n",
    "⁢\n",
    "(\n",
    "𝑝\n",
    "𝑡\n",
    ")\n",
    ",\n",
    "𝑟\n",
    "*\n",
    ")\n",
    ",\n",
    "(10)\n",
    "where \n",
    "𝒩\n",
    "𝑏\n",
    "⁢\n",
    "(\n",
    "𝑟\n",
    "*\n",
    ")\n",
    " is the set of \n",
    "𝑏\n",
    "-nearest neighbors of \n",
    "𝑟\n",
    "*\n",
    " in the memory bank. In addition, the image-wise anomaly score is computed as the maximum score over the anomaly scores calculated for every patch feature in the image.\n",
    "\n",
    "The accuracy of anomaly detection can be further improved by score-level ensemble from multiple models. To compensate different score distribution of each model, we normalize each score using the modified z-score [1], normalization is necessary to evenly fuse the score levels of each model. The anomaly score is normalized to the modified z-score [1], defined as\n",
    "\n",
    "𝑠\n",
    "¯\n",
    "𝑡\n",
    "=\n",
    "𝑠\n",
    "𝑡\n",
    "−\n",
    "𝑠\n",
    "~\n",
    "𝛽\n",
    "⋅\n",
    "𝑀\n",
    "⁢\n",
    "𝐴\n",
    "⁢\n",
    "𝐷\n",
    ",\n",
    "(11)\n",
    "where \n",
    "𝑠\n",
    "~\n",
    " and \n",
    "𝑀\n",
    "⁢\n",
    "𝐴\n",
    "⁢\n",
    "𝐷\n",
    " are the median value of the anomaly scores and the Mean Absolute Deviation over the entire dataset for training. \n",
    "𝛽\n",
    " is a constant scale factor, which is set to 1.4826 in our method, assuming the anomaly score is normally distributed.\n",
    "\n",
    "4Experiments and analysis\n",
    "4.1Experimental setup\n",
    "Dataset In this study, we used the MVTec AD [4] dataset and BTAD [26] dataset for our experiments. MVTec AD dataset is widely used as an industrial anomaly detection benchmark. It consists of 15 categories, with 3,629 training images and 1,725 test images. BTAD dataset is composed of RGB images representing three distinct industrial products. The dataset consists of 1,799 images for training and 741 images for testing. The training dataset includes only normal images, whereas the test dataset includes both normal and anomalous images. Each category in the test dataset has labels for normal and abnormal images, and anomaly ground truth mask labels for segmentation evaluation.\n",
    "\n",
    "Metrics To evaluate the performance of our proposed model, anomaly detection and segmentation performance is compared using the area under the receiver operation characteristic (AUROC) curve metric, following [8, 10, 20, 30]. For detection performance evaluation, we measure the image-level AUROC by using the model output anomaly score and the normal/abnormal labels of the test dataset. For segmentation, we measures the pixel-level AUROC using the anomaly scores obtained from the model output for all pixels and the anomaly ground truth mask labels.\n",
    "\n",
    "Method\tOurs-25%\tOurs-10%\tOurs-1%\n",
    "Detection\t99.24\t99.27\t99.49\n",
    "Segmentation\t98.01\t98.07\t98.07\n",
    "Table 1:Ablation study results on the coreset subsampling percentage for our proposed ReConPatch model with a WideResNet-50 backbone on the MVTec AD dataset.\n",
    "Dimension\t1024\t512\t256\t128\t64\n",
    "PatchCore\t99.1\t98.66\t98.45\t98.54\t97.75\n",
    "ReConPatch\t99.49\t99.56\t99.53\t99.52\t99.14\n",
    "Table 2:Ablation study results for the \n",
    "𝑓\n",
    " layer dimension on the MVTec AD dataset using PatchCore[30] and proposed ReConPatch model with a WideResNet-50 backbone.\n",
    "Metric\tDetection\tSegmentation\n",
    "WRN-50, \n",
    "𝑠\n",
    "=\n",
    "3\n",
    ", 512 dim, layer (2+3), Imagesize 224\n",
    "AUROC\t99.56\t98.07\n",
    "WRN-50, \n",
    "𝑠\n",
    "=\n",
    "5\n",
    ", 512 dim, layer (2+3), Imagesize 224\n",
    "AUROC\t98.84\t97.82\n",
    "WRN-50, \n",
    "𝑠\n",
    "=\n",
    "5\n",
    ", 512 dim, layer (1+2+3), Imagesize 224\n",
    "AUROC\t98.7\t98.18\n",
    "Table 3:Ablation study results with adding more hierarchy levels and larger patch size for our proposed ReConPatch model on the MVTec AD dataset.\n",
    "Method\t \n",
    "Class \n",
    "→\n",
    "↓\n",
    " Aug. Method \tObject\tTexture\tAverage\n",
    "w/o Aug.\t99.17\t98.96\t99.10\n",
    "PatchCore\tw/ Aug.\t94.86\t96.09\t95.48\n",
    "Diff.\t9.94\t2.87\t3.62\n",
    "w/o Aug.\t99.44\t99.81\t99.56\n",
    "ReConPatch\tw/ Aug.\t97.65\t99.47\t98.56\n",
    "Diff.\t1.79\t0.34\t1.00\n",
    "Table 4:Ablation study results for data augmentation on MVTec AD dataset using PatchCore[30] and proposed ReConPatch.\n",
    "Implementation details. For the single model, ImageNet pre-trained WideResNet-50 [44] is employed as the feature extractor. The \n",
    "𝑓\n",
    " layer output size is set to 512, and the coreset subsampling percentage is set to 1%. Our proposed ReConPatch is trained for 120 epochs per each category. Without specific instructions, hierarchy levels2 2 and 3 are used with a patch size of \n",
    "𝑠\n",
    "=\n",
    "3\n",
    " to generate the patch-level features. Particularly for the segmentation evaluation in Table 5, hierarchy levels 1, 2, and 3 were used with a patch size of \n",
    "𝑠\n",
    "=\n",
    "5\n",
    ", which is identified as the best performance through the ablation study in section 4.2. In addition, for the comparison with PNI [2] using WideResNet-101, hierarchy levels 2 and 3 were used with a patch size of \n",
    "𝑠\n",
    "=\n",
    "5\n",
    ".\n",
    "\n",
    "For the ensemble model, ImageNet pre-trained WideResNet-101 [44], ResNext-101 [40], and DenseNet-201 [16] are used as feature extractors for comparison with the PatchCore [30]. The \n",
    "𝑓\n",
    " layer output size was set to 384, and we applied a coreset subsampling with percentage of 1% to all models in the ensemble. We trained ReConPatch for 60 epochs for each category. Hierarchy levels 2 and 3 were used for feature extraction in each model, and a patch size of \n",
    "𝑠\n",
    "=\n",
    "3\n",
    " was applied to generate the patch-level features. Furthermore, to compare with PNI [2] using 480\n",
    "×\n",
    "480 image size, different parameters were applied. The \n",
    "𝑓\n",
    " layer output size was set to 512, and a patch size of \n",
    "𝑠\n",
    "=\n",
    "5\n",
    " was used. In this case, we trained each category for 120 epochs. ReConPatch was trained using AdamP [15] optimizer with a cosine annealing [23] scheduler. The learning rate was set to 1e-5 for a single model and 1e-6 for the ensemble model, with a weight decay of 1e-2. In the models using a 480\n",
    "×\n",
    "480 image size, the learning rate was specifically set to 1e-6. We provide the hyperparameter setup in Appendix B.\n",
    "\n",
    "Backbone\tWRN-101\tWRN-50\n",
    "Image size\t480\n",
    "×\n",
    "480\t480\n",
    "×\n",
    "480\t256\n",
    "×\n",
    "256\t224\n",
    "×\n",
    "224\t224\n",
    "×\n",
    "224\t224\n",
    "×\n",
    "224\t224\n",
    "×\n",
    "224\t224\n",
    "×\n",
    "224\n",
    "↓\n",
    " Class\\Method \n",
    "→\n",
    "PNI [2]\n",
    "(w/ refine) \tOurs\tCFLOW-AD [14]\tSPADE [8]\tPaDiM [10]\tPatchCore [30]\tCFA [20]\tOurs\n",
    "Bottle\t(100, 98.87)\t(100, 98.78)\t(100, 98.76)\t(-, 98.4)\t(-, 98.3)\t(100, 98.6)\t(100, -)\t(100, 98.2)\n",
    "Cable\t(99.76, 99.1)\t(99.66, 98.86)\t(97.59, 97.64)\t(-, 97.2)\t(-, 96.7)\t(99.5, 98.4)\t(99.8, -)\t(99.83, 99.3)\n",
    "Capsule\t(99.72, 99.34)\t(99.76, 99.24)\t(97.68, 98.98)\t(-, 99)\t(-, 98.5)\t(98.1, 98.8)\t(97.3, -)\t(98.8, 97.61)\n",
    "Hazelnut\t(100, 99.37)\t(100, 99.07)\t(99.98, 98.82)\t(-, 99.1)\t(-, 98.2)\t(100, 98.7)\t(100, -)\t(100, 98.94)\n",
    "Metal nut\t(100, 99.29)\t(100, 99.29)\t(99.26, 98.56)\t(-, 98.1)\t(-, 97.2)\t(100, 98.4)\t(100, -)\t(100, 95.76)\n",
    "Pill\t(96.89, 99.03)\t(96.21, 98.66)\t(96.82, 98.95)\t(-, 96.5)\t(-, 95.7)\t(96.6, 97.4)\t(97.9, -)\t(97.49, 95.35)\n",
    "Screw\t(99.51, 99.6)\t(99.84, 99.59)\t(91.89, 98.1)\t(-, 98.9)\t(-, 98.5)\t(98.1, 99.4)\t(97.3, -)\t(98.52, 98.79)\n",
    "Toothbrush\t(99.72, 99.09)\t(100, 99.16)\t(99.65, 98.56)\t(-, 97.9)\t(-, 98.8)\t(100, 98.7)\t(100, -)\t(100, 98.88)\n",
    "Transistor\t(100, 98.04)\t(100, 96.18)\t(95.21, 93.28)\t(-, 94.1)\t(-, 97.5)\t(100, 96.3)\t(100, -)\t(100, 99.65)\n",
    "Zipper\t(99.87, 99.43)\t(99.89, 99.25)\t(98.48, 98.41)\t(-, 96.5)\t(-, 98.5)\t(99.4, 98.8)\t(99.6, -)\t(99.76, 98.56)\n",
    "Object classes\t(99.55, 99.12)\t(99.54, 98.81)\t(97.66, 98.01)\t(-, 97.57)\t(-, 97.79)\t(99.17, 98.35)\t(99.19, -)\t(99.44, 98.1)\n",
    "Carpet\t(100, 99.4)\t(100, 99.29)\t(98.73, 99.23)\t(-, 97.5)\t(-, 99.1)\t(98.7, 99)\t(97.3, -)\t(99.6, 98.75)\n",
    "Grid\t(98.41, 99.2)\t(99.5, 98.73)\t(99.6, 96.89)\t(-, 93.7)\t(-, 97.3)\t(98.2, 98.7)\t(99.2, -)\t(100, 99.04)\n",
    "Leather\t(100, 99.56)\t(100, 99.48)\t(100, 99.61)\t(-, 97.6)\t(-, 99.2)\t(100, 99.3)\t(100, -)\t(100, 96.02)\n",
    "Tile\t(100, 98.4)\t(100, 97.15)\t(99.88, 97.71)\t(-, 87.4)\t(-, 94.1)\t(98.7, 95.6)\t(99.4, -)\t(99.78, 98.92)\n",
    "Wood\t(99.56, 97.04)\t(99.47, 95.16)\t(99.12, 94.49)\t(-, 88.5)\t(-, 94.9)\t(99.2, 95)\t(99.7, -)\t(99.65, 98.9)\n",
    "Texture classes\t(99.59, 98.72)\t(99.79, 97.96)\t(99.47, 97.59)\t(-, 92.94)\t(-, 96.92)\t(98.96, 97.52)\t(99.12, -)\t(99.81, 98.33)\n",
    "Average\t(99.56, 98.98)\t(99.62, 98.53)\t(98.26, 97.87)\t(85.5, 96)\t(95.3, 97.5)\t(99.1, 98.1)\t(99.2, 98.2)\t(99.56, 98.18)\n",
    "Table 5:Anomaly detection and segmentation performance on the MVTec AD dataset. (image-level AUROC, pixel-level AUROC)\n",
    "4.2Ablation study\n",
    "In this study, we aim to investigate the optimal configuration of ReConPatch through ablation studies. The first ablation was performed to determine the optimal coreset subsampling percentage. To this end, we compared anomaly detection and segmentation AUROC metrics using three subsampling percentages: 25%, 10%, and 1%, which were the same percentages used in PatchCore [30]. The pre-trained WideResNet-50 [44] backbone was used as the baseline for this experiment and the output dimension of the \n",
    "𝑓\n",
    " layer is set to 1024. The results are presented in Table 1. We observe that the subsampling percentage of 1% provides the best performance. In addition, experiments to analyze the performance according to the feature dimension were performed by changing various output dimension of the \n",
    "𝑓\n",
    " layer (1024, 512, 256, 128, and 64). The experiments were conducted with coreset subsampling set to 1%. The results are presented in Table 2, indicating that the highest performance was achieved with the dimension of 512. We note that even with 64 dimension, ReConPatch outperforms PatchCore with 1024, which supports the dimension reduction capability of our method.\n",
    "\n",
    "Table 3 shows the results of an ablation study using more hierarchy levels and larger patch size on the MVTec AD [4] dataset with our proposed ReConPatch model. This study aims to improve segmentation performance by utilizing more diverse and coarse information on the patch features. The results indicates that when the patch size is increased to 5 and hierarchy levels 1, 2, and 3 are used, the segmentation performance increased up to 98.18% with small decrease in detection performance.\n",
    "\n",
    "Real-world scenarios can present a variety of environmental conditions that can affect the quality of images. These conditions may include geometric changes, lighting changes, defocusing, and other factors that can impact the accuracy and reliability of image data. Table 4 shows that ReConPatch is robust to these environmental changes by learning patch-level feature representations. To simulate real-world scenarios, we randomly applied rotation, translation, color jitter (brightness and contrast), and Gaussian blur. While PatchCore’s image-level AUROC decreased to 3.62 under these conditions, ReConPatch’s only slightly decreased to 1.0.\n",
    "\n",
    "Ensemble\n",
    "Backbone \tWRN-101 & RNext-101 & DenseN-201\n",
    "Image size\t480\n",
    "×\n",
    "480\t480\n",
    "×\n",
    "480\t320\n",
    "×\n",
    "320\t320\n",
    "×\n",
    "320\n",
    "Method\t \n",
    "PNI [2]\n",
    "(w/ refine) \tOurs\t \n",
    "PatchCore\n",
    "[30] \tOurs\n",
    "Detection\t99.63\t99.72\t99.6\t99.67\n",
    "Segmentation\t99.06\t98.67\t98.2\t98.36\n",
    "Table 6:Comparison of ensemble model anomaly detection (image-level AUROC) and segmentation (pixel-level AUROC) performance on the MVTec AD dataset.\n",
    "Class\tVT-ADL [26]\tSPADE [8]\tPaDiM [10]\tFastFlow [43]\tPyramidFlow [21]\tCFA [20]\tRD4AD [11]\tRD++ [37]\tPNI [2]\tPatchCore [30]\tOurs\n",
    "1\t(97.6, 99)\t(91.4, 97.3)\t(99.8, 97)\t(99.4, 97.1)\t(100, 97.4)\t(98.1, 95.9)\t(96.3, 96.6)\t(96.8, 96.2)\t(-, 97.4)\t(98, 96.9)\t(99.7, 96.8)\n",
    "2\t(71, 94)\t(71.4, 94.4)\t(82, 96)\t(82.4, 93.6)\t(88.2, 97.6)\t(85.5, 96)\t(86.6, 96.7)\t(90.1, 96.4)\t(-, 97)\t(81.6, 95.8)\t(87.7, 96.6)\n",
    "3\t(82.6, 77)\t(99.9, 99.1)\t(99.4, 98.8)\t(91.1, 98.3)\t(99.3, 98.1)\t(99, 98.6)\t(100, 99.7)\t(100, 99.7)\t(-, 99)\t(99.8, 99.1)\t(100, 99)\n",
    "Avg.\t(83.7, 90)\t(87.6, 96.9)\t(93.7, 97.3)\t(91, 96.3)\t(95.8, 97.7)\t(94.2, 96.8)\t(94.3, 97.7)\t(95.6, 97.4)\t(-, 97.8)\t(93.1, 97.3)\t(95.8, 97.5)\n",
    "Table 7:Anomaly detection and segmentation performance on the BTAD [26] dataset. (image-level AUROC, pixel-level AUROC)\n",
    "4.3Anomaly detection on MVTec AD\n",
    "In this section, we evaluate the anomaly detection performance of our proposed method on the MVTec AD dataset by comparing it with previous works that used the same pre-trained model and image size [8, 10, 20, 30]. We also include the performance of concurrent methods PNI [2] and CFLOW-AD [14] in Tables 5. In case of PNI [2], a WideResNet-101 model with an image size of 480\n",
    "×\n",
    "480 was used. To improve its performance, a refinement network was included, which was trained in a supervised manner using artificially created defect dataset. For CFLOW-AD [14], a WideResNet-50 model with an image size of 256\n",
    "×\n",
    "256 is used. The evaluation results used in CFLOW-AD were the best performances obtained for each category when using the image size of 256\n",
    "×\n",
    "256.\n",
    "\n",
    "For the single-model performance comparison, we performed the same pre-processing as described in previous work [8, 10, 20, 30]. Specifically, we resized each image to 256\n",
    "×\n",
    "256 and then center-cropped to 224\n",
    "×\n",
    "224. For the ensemble model, the same pre-processing was used as in [30], each image was resized to 366\n",
    "×\n",
    "366 and then center-cropped to 320\n",
    "×\n",
    "320. In addition, to compare with PNI [2], we resized each image to 512\n",
    "×\n",
    "512 and then center-cropped to 480\n",
    "×\n",
    "480. No data augmentation was applied to any category.\n",
    "\n",
    "The performance of the ReConPatch in Tables 5 was obtained using 1% coreset subsampling and \n",
    "𝑓\n",
    " layer dimensions of 512, which is determined according to Table 2. Table 5 compares the anomaly detection and segmentation performance of a single model for each category of the MVTec AD [4] dataset, evaluated with image-level AUROC. Our proposed ReConPatch achieved an image-level AUROC of 99.56%, which outperformed CFA [20] (at 99.3%). Furthermore, ReConPatch provided higher performance than the state-of-the-art PNI [2] with WideResNet-101 [44], which achieved the performance of 99.62%.\n",
    "\n",
    "Our proposed approach focused on improving the anomaly detection performance. As a result, the segmentation performance may not be as high as its detection performance. However, we achieved a higher performance of 98.18% compared to PatchCore [30], indicating that the addition of ReConPatch feature in the \n",
    "𝑓\n",
    " layer contributed to the improved segmentation performance.\n",
    "\n",
    "Refer to caption\n",
    "Figure 3:An illustrative comparison of features mapped by (a) PatchCore and (b) (c) (d) ReConPatch using the MVTec AD dataset. The scatter plot describes the feature space of each method, colored according to the pixel position.\n",
    "Refer to caption\n",
    "Figure 4:The histogram of the anomaly score of the normal and abnormal data for the bottle class. ReConPatch shows high discriminability, as shown in \n",
    "𝑑\n",
    "′\n",
    " measure.\n",
    "Refer to caption\n",
    "Figure 5:Examples of images with anomalies (top) and measured anomaly score maps (bottom) on MVTec AD dataset. The orange line depicts the ground truth of the anomalies and the green line depicts thresholds optimizing F1 scores of anomaly segmentation. The green star indicates the maximal location of the anomaly score in the heatmap.\n",
    "Table 6 presents the performance of our ensemble model, which was evaluated using the modified z-score in Eq. 11 for each output from WideResNet-101 [44], ResNext-101 [40], and DenseNet-201 [16] models. Our model achieved state-of-the-art performance in anomaly detection task with AUROC of 99.72% on the MVTec AD dataset using an image size of 480\n",
    "×\n",
    "480. We note that our model still outperforms the PNI [2] using a smaller image size of 320\n",
    "×\n",
    "320, achieving an AUROC of 99.67% compared to AUROC of 99.63% Furthermore, we outperformed PatchCore [30] in terms of anomaly segmentation performance, with an improved performance of 98.36% AUROC.\n",
    "\n",
    "4.4Anomaly detection on BTAD\n",
    "To verify the capability of anomaly detection and segmentation in other dataset, we compare the performance of our model with contemporary methods using BTAD dataset [26]. For BTAD dataset, we use the pre-trained WideResNet-101 model as a feature extractor and image size of 480\n",
    "×\n",
    "480 for ReConPatch, which achieve our best performance. Table 7 shows the image-level AUROC and the pixel-level AUROC on BTAD dataset. Our model achieves a state-of-the-art performance in anomaly detection, with an AUROC of 95.8%. Furthermore, in anomaly segmentation, our model outperforms PatchCore [30] with a higher AUROC of 97.5%.\n",
    "\n",
    "4.5Qualitative analysis\n",
    "To assess the impact of ReConPatch learning on the feature space, we contrast the feature space of PatchCore and ReConPatch using the MVTec AD dataset. Our visualization, depicted in Figure 3, employs UMAP [25] for effective 2D representation of high-dimensional patch features, with color coding indicating spatial positions. The visualization attests that ReConPatch’s training encourages proximity of features with similar positions. Building on findings in prior research [2, 14], which demonstrated the value of positional information, we hypothesize that ReConPatch’s performance enhancement arises from implicit positional information learning. We also visualize the feature map along the training, which indicates the features are trained to map similar position to be gather.\n",
    "\n",
    "ReConPatch’s reconfigured feature space yields more distinct histogram distributions of image-level anomaly scores compared to PatchCore. In Figure 4, we observe this effect on the MVTec AD dataset’s bottle class. ReConPatch compresses the score distribution for normal data while pushing the abnormal data’s distribution further from the normal one, a contrast to PatchCore [30]. We gauge the distribution separability using the \n",
    "𝑑\n",
    "′\n",
    " discriminability index [35] between normal and abnormal data:\n",
    "\n",
    "𝑑\n",
    "′\n",
    "=\n",
    "|\n",
    "𝜇\n",
    "𝑎\n",
    "⁢\n",
    "𝑏\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑚\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    "−\n",
    "𝜇\n",
    "𝑛\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑚\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    "|\n",
    "(\n",
    "𝜎\n",
    "𝑎\n",
    "⁢\n",
    "𝑏\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑚\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    "2\n",
    "+\n",
    "𝜎\n",
    "𝑛\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑚\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑙\n",
    "2\n",
    ")\n",
    "/\n",
    "2\n",
    ".\n",
    "(12)\n",
    "Here, the patch features mirror those of locally aware patch features in PatchCore. ReConPatch, as detailed in Section 3.2, leverages target-oriented features through patch-level representation training, enhancing discrimination between normal and abnormal attributes. Performance-wise (Table 5), ReConPatch achieves an image-level AUROC of 99.56\n",
    "\n",
    "We present anomaly score maps overlaid on input images (Figure 5) with ground truth annotations. Higher values in the anomaly map indicate probable anomalies. A threshold optimized via F1 scores governs the green line. Our analysis focuses on 4 superior classes (cable, transistor, tile, wood) and 3 inferior classes (metal nut, pill, leather). Despite intricate ground truth cases, ReConPatch consistently identifies anomaly locations. While inferior class anomaly maps may exhibit noise, the green star pinpointing maximal anomaly score aligns with ground truth anomalies, affirming our method’s robust performance in anomaly detection.\n",
    "\n",
    "5Conclusion\n",
    "In this paper, we introduce the ReConPatch to learn a target-oriented representation space, which can effectively distinguish the anomalies from the normal dataset. ReConPatch effectively trains the representation by applying the metric learning with softly guided by the similarity over the nominal features. Applying the contrastive learning with two similarity based pseudo soft labels, ReConPatch shows the state-of-the-art performance on the MVTec anomaly detection dataset. We also provide the anomaly detection performance on the additional BTAD dataset, where ReConPatch also achieves the best performance. We believe that ReConPatch would contribute to the improvements in anomaly detection since it shows high performance without extensive data augmentation and enables dimension reduction without significant loss of performance. Furthermore, we expect to improve the performance in the pixel-level abnormal detection by considering the correlation among the neighboring features.\n",
    "\n",
    "References\n",
    "[1]\n",
    "Vaibhav Aggarwal, Vaibhav Gupta, Prayag Singh, Kiran Sharma, and Neetu Sharma.Detection of spatial outlier by using improved z-score test.pages 788–790, 2019.\n",
    "[2]\n",
    "Jaehyeok Bae, Jae-Han Lee, and Seyun Kim.Pni: industrial anomaly detection using position and neighborhood information.pages 6373–6383, 2023.\n",
    "[3]\n",
    "Liron Bergman, Niv Cohen, and Yedid Hoshen.Deep nearest neighbor anomaly detection.arXiv preprint arXiv:2002.10445, 2020.\n",
    "[4]\n",
    "Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger.Mvtec ad–a comprehensive real-world dataset for unsupervised anomaly detection.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9592–9600, 2019.\n",
    "[5]\n",
    "Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger.Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4183–4192, 2020.\n",
    "[6]\n",
    "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.A simple framework for contrastive learning of visual representations.In International conference on machine learning, pages 1597–1607. PMLR, 2020.\n",
    "[7]\n",
    "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.Describing textures in the wild.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3606–3613, 2014.\n",
    "[8]\n",
    "Niv Cohen and Yedid Hoshen.Sub-image anomaly detection with deep pyramid correspondences.arXiv preprint arXiv:2005.02357, 2020.\n",
    "[9]\n",
    "Diana Davletshina, Valentyn Melnychuk, Viet Tran, Hitansh Singla, Max Berrendorf, Evgeniy Faerman, Michael Fromm, and Matthias Schubert.Unsupervised anomaly detection for x-ray images.arXiv preprint arXiv:2001.10883, 2020.\n",
    "[10]\n",
    "Thomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier.Padim: a patch distribution modeling framework for anomaly detection and localization.In Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10–15, 2021, Proceedings, Part IV, pages 475–489. Springer, 2021.\n",
    "[11]\n",
    "Hanqiu Deng and Xingyu Li.Anomaly detection via reverse distillation from one-class embedding.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9737–9746, 2022.\n",
    "[12]\n",
    "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.Imagenet: A large-scale hierarchical image database.In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.\n",
    "[13]\n",
    "Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.Density estimation using real nvp.In 5th International Conference on Learning Representations, 2017.\n",
    "[14]\n",
    "Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka.Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows.In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 98–107, 2022.\n",
    "[15]\n",
    "Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, and Jung-Woo Ha.Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights.arXiv preprint arXiv:2006.08217, 2020.\n",
    "[16]\n",
    "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.Densely connected convolutional networks.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017.\n",
    "[17]\n",
    "Jeff Johnson, Matthijs Douze, and Hervé Jégou.Billion-scale similarity search with gpus.IEEE Transactions on Big Data, pages 1–1, 2019.\n",
    "[18]\n",
    "Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.Embedding transfer with label relaxation for improved metric learning.pages 3967–3976, June 2021.\n",
    "[19]\n",
    "Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.Self-taught metric learning without labels.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7431–7441, 2022.\n",
    "[20]\n",
    "Sungwook Lee, Seunghyun Lee, and Byung Cheol Song.Cfa: Coupled-hypersphere-based feature adaptation for target-oriented anomaly localization.IEEE Access, 10:78446–78454, 2022.\n",
    "[21]\n",
    "Jiarui Lei, Xiaobo Hu, Yue Wang, and Dong Liu.Pyramidflow: High-resolution defect contrastive localization using pyramid normalizing flow.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14143–14152, 2023.\n",
    "[22]\n",
    "Christopher Liao, Theodoros Tsiligkaridis, and Brian Kulis.Supervised metric learning to rank for retrieval via contextual similarity optimization.arXiv preprint arXiv:2210.01908, 2022.\n",
    "[23]\n",
    "Ilya Loshchilov and Frank Hutter.Sgdr: Stochastic gradient descent with warm restarts.arXiv preprint arXiv:1608.03983, 2016.\n",
    "[24]\n",
    "TorchVision maintainers and contributors.Torchvision: Pytorch’s computer vision library.https://github.com/pytorch/vision, 2016.\n",
    "[25]\n",
    "Leland McInnes, John Healy, and James Melville.Umap: Uniform manifold approximation and projection for dimension reduction.arXiv preprint arXiv:1802.03426, 2018.\n",
    "[26]\n",
    "Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti.Vt-adl: A vision transformer network for image anomaly detection and localization.In 2021 IEEE 30th International Symposium on Industrial Electronics (ISIE), pages 01–06. IEEE, 2021.\n",
    "[27]\n",
    "Duc Tam Nguyen, Zhongyu Lou, Michael Klar, and Thomas Brox.Anomaly detection with multiple-hypotheses predictions.In International Conference on Machine Learning, pages 4800–4809. PMLR, 2019.\n",
    "[28]\n",
    "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.Pytorch: An imperative style, high-performance deep learning library, 2019.\n",
    "[29]\n",
    "Stanislav Pidhorskyi, Ranya Almohsen, and Gianfranco Doretto.Generative probabilistic novelty detection with adversarial autoencoders.Advances in neural information processing systems, 31, 2018.\n",
    "[30]\n",
    "Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, and Peter Gehler.Towards total recall in industrial anomaly detection.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14318–14328, 2022.\n",
    "[31]\n",
    "Marco Rudolph, Bastian Wandt, and Bodo Rosenhahn.Same same but differnet: Semi-supervised defect detection with normalizing flows.In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 1907–1916, 2021.\n",
    "[32]\n",
    "Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Müller, and Marius Kloft.Deep one-class classification.In International conference on machine learning, pages 4393–4402. PMLR, 2018.\n",
    "[33]\n",
    "Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli.Adversarially learned one-class classifier for novelty detection.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3379–3388, 2018.\n",
    "[34]\n",
    "Mayu Sakurada and Takehisa Yairi.Anomaly detection using autoencoders with nonlinear dimensionality reduction.In Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis, pages 4–11, 2014.\n",
    "[35]\n",
    "Adrian J Simpson and Mike J Fitter.What is the best index of detectability?Psychological Bulletin, 80(6):481, 1973.\n",
    "[36]\n",
    "Samarth Sinha, Han Zhang, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, and Augustus Odena.Small-GAN: Speeding up GAN training using core-sets.119:9005–9015, 13–18 Jul 2020.\n",
    "[37]\n",
    "Tran Dinh Tien, Anh Tuan Nguyen, Nguyen Hoang Tran, Ta Duc Huy, Soan Duong, Chanh D Tr Nguyen, and Steven QH Truong.Revisiting reverse distillation for anomaly detection.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24511–24520, 2023.\n",
    "[38]\n",
    "Guido Van Rossum and Fred L. Drake.Python 3 Reference Manual.CreateSpace, Scotts Valley, CA, 2009.\n",
    "[39]\n",
    "Ross Wightman.Pytorch image models.https://github.com/rwightman/pytorch-image-models, 2019.\n",
    "[40]\n",
    "Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.Aggregated residual transformations for deep neural networks.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492–1500, 2017.\n",
    "[41]\n",
    "Minghui Yang, Peng Wu, and Hui Feng.Memseg: A semi-supervised method for image surface defect detection using differences and commonalities.Engineering Applications of Artificial Intelligence, 119:105835, 2023.\n",
    "[42]\n",
    "Jihun Yi and Sungroh Yoon.Patch svdd: Patch-level svdd for anomaly detection and segmentation.In Proceedings of the Asian Conference on Computer Vision, 2020.\n",
    "[43]\n",
    "Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, and Liwei Wu.Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows.arXiv preprint arXiv:2111.07677, 2021.\n",
    "[44]\n",
    "Sergey Zagoruyko and Nikos Komodakis.Wide residual networks.arXiv preprint arXiv:1605.07146, 2016.\n",
    "[45]\n",
    "Vitjan Zavrtanik, Matej Kristan, and Danijel Skočaj.Draem-a discriminatively trained reconstruction embedding for surface anomaly detection.In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8330–8339, 2021.\n",
    "[46]\n",
    "Hui Zhang, Zuxuan Wu, Zheng Wang, Zhineng Chen, and Yu-Gang Jiang.Prototypical residual networks for anomaly detection and localization.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16281–16291, 2023.\n",
    "[47]\n",
    "Xuan Zhang, Shiyu Li, Xi Li, Ping Huang, Jiulong Shan, and Ting Chen.Destseg: Segmentation guided denoising student-teacher for anomaly detection.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3914–3923, 2023.\n",
    "Supplementary: ReConPatch : Contrastive Patch Representation Learning for Industrial Anomaly Detection\n",
    "Appendix AImplementation details\n",
    "ReConPatch was implemented with Python 3.7 [38] and PyTorch 1.9 [28]. Experiments are run on Nvidia GeForce GTX 3090 GPU. We used ImageNet-pretrained models from torchvision [24] and the PyTorch Image Models repository [39]. By default, following [8] and [10], ReConPatch uses WideResNet50-backbone and WideResNet101-backbone[44] for direct comparability. Patch-level features are taken from feature map aggregation of the final outputs in \n",
    "𝑓\n",
    " layer of ReConPatch. We use faiss [17] to compute all nearest neighbor retrieval and distance computations same as PatchCore[30].\n",
    "\n",
    "Appendix BAdditional experiments on MVTec AD\n",
    "This section contains the details of setting up hyper-parameters and experiments with the projection layer of ReConPatch on MVTec AD[4] dataset. We experimented to find the optimal hyper-parameters with the following values: \n",
    "𝑘\n",
    " value of k-nearest neighborhood for calculating contextual similarity, the repelling margin \n",
    "𝑚\n",
    " in RC loss, and applying ratio \n",
    "𝛼\n",
    " between pairwise and contextual similarity. Additionally, to verify the effectiveness of projection layer \n",
    "𝑔\n",
    ", we tested our proposed method without projection layer \n",
    "𝑔\n",
    ".\n",
    "\n",
    "All experiments have the same conditions. Each images are resized to 256\n",
    "×\n",
    "256 and center-cropped to 224\n",
    "×\n",
    "224 and ImageNet-pretrained WideResNet50-backbone[44] from torchvision[24] was employed as the feature extractor. The \n",
    "𝑓\n",
    " layer output size was set to 512, and the coreset subsampling percentage was set to 1%. The ReConPatch was trained for 120 epochs per each class on MVTec AD [4] with AdamP[15] optimizer with cosine annealing [23] scheduler.\n",
    "\n",
    "B.1The \n",
    "𝑘\n",
    " value of k-nearest neighbor for contextual similarity\n",
    "We tested several \n",
    "𝑘\n",
    " - nearest neighborhood value \n",
    "𝑘\n",
    " for finding the best performance of anomaly detection on MVTec AD[4] dataset. In Eq. 2, smaller \n",
    "𝑘\n",
    " values will use features that are closer together to determine contextual similarity, while larger \n",
    "𝑘\n",
    " values will use features that are further apart in the embedding space to determine contextual similarity. To find an optimal \n",
    "𝑘\n",
    " value, we fixed other hyper-parameters \n",
    "𝑚\n",
    "=\n",
    "1\n",
    ", \n",
    "𝛼\n",
    "=\n",
    "0.5\n",
    ". Table S1 shows the results of experiments.\n",
    "\n",
    "ReConPatch (WRS-50, 224\n",
    "×\n",
    "224, \n",
    "𝛼\n",
    "=\n",
    "0.5\n",
    ", \n",
    "𝑚\n",
    "=\n",
    "1.0\n",
    ")\n",
    "𝑘\n",
    " value\t3\t5\t7\t10\t15\t20\n",
    "Detection\t99.5\t99.56\t99.55\t99.54\t99.5\t99.49\n",
    "Segmentation\t98.09\t98.07\t98.07\t98.07\t98.04\t98.07\n",
    "Table S1:Results of anomaly detection(image-level AUROC) and segmentation(pixel-level AUROC) with various \n",
    "𝑘\n",
    "-nearest neighborhood on MVTec AD dataset. (\n",
    "𝛼\n",
    "=\n",
    "0.5\n",
    ", \n",
    "𝑚\n",
    "=\n",
    "1.0\n",
    ").\n",
    "B.2The repelling margin \n",
    "𝑚\n",
    " of relaxed contrastive loss\n",
    "We tested margin value \n",
    "𝑚\n",
    " in Eq. 7 repelling term of relaxed contrastive loss with various \n",
    "𝑚\n",
    " values to obtain the performance of anomaly detection on MVTec AD[4]. All other hyper-parameters are fixed(\n",
    "𝑘\n",
    "=\n",
    "5\n",
    ", \n",
    "𝛼\n",
    "=\n",
    "0.5\n",
    "). Table S2 is the results of experiments. Changing the margin \n",
    "𝑚\n",
    " has no effect on performance of anomaly detection(image-level AUROC) but it does slightly affect on segmentation(pixel-level AUROC). So we use the repelling margin \n",
    "𝑚\n",
    " to 1.0.\n",
    "\n",
    "ReConPatch (WRS-50, 224\n",
    "×\n",
    "224, \n",
    "𝑘\n",
    "=\n",
    "5\n",
    ", \n",
    "𝛼\n",
    "=\n",
    "0.5\n",
    ")\n",
    "m value\t0.5\t1\t1.5\t2\n",
    "Detection\t99.55\t99.56\t99.55\t99.5\n",
    "Segmentation\t98.04\t98.07\t98.05\t98.04\n",
    "Table S2:Results of anomaly detection(image-level AUROC) and segmentation(pixel-level AUROC) with various repelling margin \n",
    "𝑚\n",
    " values of RC loss. (\n",
    "𝑘\n",
    "=\n",
    "5\n",
    ", \n",
    "𝛼\n",
    "=\n",
    "0.5\n",
    ").\n",
    "B.3The applying ratio \n",
    "𝛼\n",
    " between pairwise and contextual similarity\n",
    "We tested \n",
    "𝛼\n",
    " in Eq. 6 which the ratio of linear combination between pairwise and contextual similarity within range \n",
    "[\n",
    "0\n",
    ",\n",
    "1\n",
    "]\n",
    ". If \n",
    "𝛼\n",
    " is 0 then only use contextual similarity, and if \n",
    "𝛼\n",
    " set to 1.0 then use pairwise similarity only. Table S3 shows the best detection(image-level AUROC) performance when using pairwise and contextual similarity with same ratio.\n",
    "\n",
    "ReConPatch (WRS-50, 224\n",
    "×\n",
    "224, \n",
    "𝑘\n",
    "=\n",
    "5\n",
    ", \n",
    "𝑚\n",
    "=\n",
    "1.0\n",
    ")\n",
    "𝛼\n",
    "0.0\t0.25\t0.5\t0.75\t1.0\n",
    "Detection\t99.51\t99.51\t99.56\t99.52\t99.5\n",
    "Segmentation\t98.02\t98.07\t98.07\t98.1\t98.12\n",
    "Table S3:Results of anomaly detection(image-level AUROC) and segmentation(pixel-level AUROC) with various \n",
    "𝛼\n",
    " values within range \n",
    "[\n",
    "0\n",
    ",\n",
    "1\n",
    "]\n",
    " of ReConPatch. (\n",
    "𝑘\n",
    "=\n",
    "5\n",
    ", \n",
    "𝑚\n",
    "=\n",
    "1.0\n",
    ").\n",
    "B.4The projection layer \n",
    "𝑔\n",
    "We tested with linear projection layer \n",
    "𝑔\n",
    ", without projection layer \n",
    "𝑔\n",
    ". Fig. S1 describes the visualization of patch features obtained by ReConPatch which trained with projection layer \n",
    "𝑔\n",
    " and trained without projection layer. For effective visualization of high dimensional feature vectors, we map the patch features into 2-dimensional space using UMAP[25]. With visualization result, we verify that the projection layer \n",
    "𝑔\n",
    " of ReConPatch encourages the features with similar position together. We can also verify that using the projection layer has better anomaly detection performance than not using the projection layer by the results in Table S4.\n",
    "\n",
    "ReConPatch (WRS-50, 224\n",
    "×\n",
    "224, \n",
    "𝑘\n",
    "=\n",
    "5\n",
    ", \n",
    "𝑚\n",
    "=\n",
    "1.0\n",
    ", \n",
    "𝛼\n",
    "=\n",
    "1.0\n",
    ")\n",
    "Projection layer\tw/o Proj. Layer\tw/ Proj. Layer\n",
    "Detection\t99.42\t99.56\n",
    "Table S4:Results of Detection AUROC with and without projection layer \n",
    "𝑔\n",
    " on the MVTec AD dataset[4] using our proposed ReConPatch model with a WideResNet-50 backbone[44], 224\n",
    "×\n",
    "224 input size, 512 dimensional \n",
    "𝑓\n",
    " layer, \n",
    "𝑘\n",
    "=\n",
    "5\n",
    ", \n",
    "𝑚\n",
    "=\n",
    "1.0\n",
    ", \n",
    "𝛼\n",
    "=\n",
    "1.0\n",
    " and 1% coreset sampling.\n",
    "Refer to caption\n",
    "Figure S1:An illustrative comparison of features mapped by ReConPatch with projection layer (a) and without projection layer (b) using the MVTec AD dataset[4]. The scatter plot describes the feature space of each method, colored according to the pixel position.\n",
    "Appendix CPerformance of ReConPatch\n",
    "C.1Applying data augmenatation\n",
    "In the industrial domain, product images are collected in a well-controlled environment, but uncontrollable change in the environmental conditions at the time of acquisition can cause variation in the image data. This can lead to variation between normal image data, which can adversely affect the performance of industrial anomaly detection. To verify that ReConPatch is adaptable to environmental changes, we applied various data augmentation method to evaluate its anomaly detection performance. We randomly applied rotation(\n",
    "±\n",
    "5\n",
    "⁢\n",
    "𝑑\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑔\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑒\n",
    "), translation(\n",
    "±\n",
    "1\n",
    "%\n",
    "), color jitter(brightness and contrast \n",
    "±\n",
    "30\n",
    "%\n",
    "), and gaussian blur(\n",
    "𝜎\n",
    "=\n",
    "[\n",
    "0.1\n",
    ",\n",
    "1.0\n",
    "]\n",
    ") which could occur in the real world. Table S5 shows the detail performance comparison of PatchCore[30] and ReConPatch with data augmentation on MVTec AD dataset[4].\n",
    "\n",
    "Method\tPatchCore\tReConPatch\n",
    "↓\n",
    " Class\\Aug. Method \n",
    "→\n",
    "w/o Aug.\tw/ Aug.\tDiff\tw/o Aug.\tw/ Aug.\tDiff\n",
    "Bottle\t100\t99.68\t0.32\t100\t100\t0\n",
    "Cable\t99.5\t96.59\t2.91\t99.83\t99.01\t0.82\n",
    "Capsule\t98.1\t82.73\t15.37\t98.8\t95.29\t3.51\n",
    "Hazelnut\t100\t100\t0\t100\t100\t0\n",
    "Metal nut\t100\t98.92\t1.08\t100\t99.9\t0.1\n",
    "Pill\t96.6\t92.09\t4.51\t97.49\t94.73\t2.76\n",
    "Screw\t98.1\t89.75\t8.35\t98.52\t89.96\t8.56\n",
    "Toothbrush\t100\t95\t5\t100\t99.17\t0.83\n",
    "Transistor\t100\t98.37\t1.63\t100\t99.33\t0.67\n",
    "Zipper\t99.4\t95.51\t3.89\t99.76\t99.13\t0.63\n",
    "Object classes\t99.17\t94.86\t4.31\t99.44\t97.65\t1.79\n",
    "Carpet\t98.7\t96.47\t2.23\t99.6\t99.04\t0.56\n",
    "Grid\t98.2\t87.89\t10.31\t100\t98.5\t1.5\n",
    "Leather\t100\t100\t0\t100\t100\t0\n",
    "Tile\t98.7\t96.64\t2.06\t99.78\t100\t-0.22\n",
    "Wood\t99.2\t99.47\t-0.27\t99.65\t99.82\t-0.17\n",
    "Texture classes\t98.96\t96.09\t2.87\t99.81\t99.47\t0.34\n",
    "Average\t99.1\t95.48\t3.62\t99.56\t98.56\t1.0\n",
    "Table S5:Anomaly detection performance with data augmentation(random rotate, translate, brightness, contrast and gaussian blur) on the MVTec AD dataset[4]\n",
    "C.2Inference time\n",
    "We measured inference time and memory usage of ReConPatch and PatchCore[30] with Intel Xeon Gold 6240 CPU and Nvidia GeForce GTX 3090 GPU. Test images are resized to 256\n",
    "×\n",
    "256 and center cropped to 224\n",
    "×\n",
    "224 on MVTec AD dataset [4]. The target embed dimension was set to 512 in both PatchCore[30] and ReConPatch. We experimented for 30 iterations with 10 warm-up times to measure the inference time of PatchCore[30] and our proposed ReConPatch. In Table S6, the average inference time of PatchCore[30] and ReConPatch are not significantly different.\n",
    "\n",
    "Method\tAvg. inference time(msec)\n",
    "PatchCore\t37.89\n",
    "ReConPatch\t38.09\n",
    "Table S6:Inference time comparison between PatchCore[30] and ReConPatch on MVTec AD dataset.\n",
    "C.3Memory efficiency\n",
    "Our proposed ReConPatch algorithm uses a single linear layer which trained with local patch features of normal samples. In Eq. S1, memory usage of memory bank \n",
    "𝑀\n",
    "𝑐\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑠\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑡\n",
    " is affected by dimension of \n",
    "𝑓\n",
    " layer \n",
    "𝑓\n",
    "𝑑\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑚\n",
    " and percentage of coreset subsampling \n",
    "𝑐\n",
    "𝑝\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑐\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑔\n",
    "⁢\n",
    "𝑒\n",
    ". \n",
    "𝐻\n",
    "𝑓\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑒\n",
    " and \n",
    "𝑊\n",
    "𝑓\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑠\n",
    " are size of outputs in blocks 2 and 3. The results of anomaly detection and segmentation with various \n",
    "𝑓\n",
    " layer dimension shows in Table 2 and Fig. S2. In Fig. S2, the performance of ReConPatch for anomaly detection (image-level AUROC) and segmentation (pixel-level AUROC) is better than PatchCore[30] with same memory bank size.\n",
    "\n",
    "𝑀\n",
    "𝑐\n",
    "⁢\n",
    "𝑜\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑠\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑡\n",
    "=\n",
    "𝑓\n",
    "𝑑\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑚\n",
    "*\n",
    "𝐻\n",
    "𝑓\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑠\n",
    "*\n",
    "𝑊\n",
    "𝑓\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑢\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑠\n",
    "*\n",
    "𝑁\n",
    "𝑡\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑖\n",
    "⁢\n",
    "𝑛\n",
    "*\n",
    "𝑐\n",
    "𝑝\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑟\n",
    "⁢\n",
    "𝑐\n",
    "⁢\n",
    "𝑒\n",
    "⁢\n",
    "𝑛\n",
    "⁢\n",
    "𝑡\n",
    "⁢\n",
    "𝑎\n",
    "⁢\n",
    "𝑔\n",
    "⁢\n",
    "𝑒\n",
    "(S1)\n",
    "Refer to caption\n",
    "Figure S2:The results of Detection AUROC and Segmentation AUROC for the \n",
    "𝑓\n",
    " layer dimension on the MVTec AD dataset[4]. Image size is 224\n",
    "×\n",
    "224, \n",
    "𝑘\n",
    "=\n",
    "5\n",
    ", \n",
    "𝑚\n",
    "=\n",
    "1.0\n",
    ", \n",
    "𝛼\n",
    "=\n",
    "1.0\n",
    " and coreset subsampling rate is 1%\n",
    "C.4Anomaly score maps\n",
    "We provide the examples of the anomaly score map of MVTec AD dataset[4] and BTAD[26] dataset along with ground truth (orange line) overlaid input images in Fig.S3, S4. The anomaly map indicates the regions of the input image where ReConPatch has detected anomalies, with higher values indicating a higher likelihood of an anomaly being present. The green line indicates the threshold which is optimized by the F1 scores of anomaly segmentation, and the orange line indicates the ground truth of the anomalies.\n",
    "\n",
    "Refer to caption\n",
    "Refer to caption\n",
    "Figure S3:Examples of images with anomalies (top) and measured anomaly score maps (bottom) on MVTec AD dataset[4]. The orange line depicts the ground truth of the anomalies and the green line depicts thresholds optimizing F1 scores of anomaly segmentation. The green star indicates the maximal location of the anomaly score in the heatmap.\n",
    "Refer to caption\n",
    "Figure S4:Examples of images with anomalies (top) and measured anomaly score maps (bottom) on BTAD dataset[26]. The orange line depicts the ground truth of the anomalies and the green line depicts thresholds optimizing F1 scores of anomaly segmentation. The green star indicates the maximal location of the anomaly score in the heatmap.\n",
    "Appendix DRefinement Network for Fine-grained Anomaly Localization\n",
    "This section describes the improved performance through the combination of ReConPatch and the refinement network. As the patch-level score map of ReConPatch is obtained by aggregating features of intermediate layers, it requires the resolution matching between the score map and the input image. To match the resolution, ReConPatch utilizes the bilinear upsampling and the Gaussian smoothing, but this approach fails to represent abnormal regions with the desired level of detail. To address this issue, similar to [2], we utilize refinement network to refine anomaly score map of ReConPatch in the MVTec AD dataset. Inspired by [45, 41, 46], we artificially generate synthetic abnormal images from normal images and DTD dataset [7] and utilize focal loss and L1 loss to minimize difference between synthetic anomaly mask and predicted pixel-wise score map for training refinement network with supervised learning.\n",
    "\n",
    "D.1Implementation Details\n",
    "Refinement Network contains two residual blocks and several parallel Atrous convolution with different rates (Atrous Spatial Pyramid Pooling [47]). We train the network using the SGD optimizer for 500 epochs with the batch size of 16, the learning rate of \n",
    "10\n",
    "−\n",
    "2\n",
    ", the momentum of 0.9 and the weight decay of \n",
    "10\n",
    "−\n",
    "4\n",
    ". To compare with the ensemble model of ReConPatch, each image was resized and center-cropped to the size described in Section 4.3.\n",
    "\n",
    "D.2Abnormal Image Simulation\n",
    "To train the refinement network, we require the abnormal image and the corresponding mask that indicates the anomaly region. Due to the absence of the abnormal images for the anomaly detection task, we utilize the synthetic abnormal image for the training. In previous work [45, 41, 46], the synthetic images are generated by mixing the random sampled mask of the normal image with images from the external dataset [7]. This abnormal image simulation method works as simulating the textural anomalies. In addition to simulating textural anomalies, MemSeg [41] proposes to simulate the structural anomalies by using the randomly permuted image patches instead of the external image. We compare the performance of refinement network trained on the abnormal image simulation with texture and structure respectively. Table S7 shows the average performance over the classes of MVTec AD dataset, which indicates that using the structural anomalies gives the best performance. When analyzing structure and texture from the perspective of anomaly score maps, we observed that the deviation in anomaly scores for structure is smaller than the texture. We evaluated that the smaller deviation cause the better performance for discrimination based on image-level anomaly score.\n",
    "\n",
    "Table S7:Evaluating the components of abnormal simulation strategy on the MVTec AD benchmark\n",
    "ReConPatch (WRS-50, 224x224)\n",
    "Method\tw/o Refinement\tw/ Refinement [47]\n",
    "Simulation type\t-\tTexture [45]\tStructure [41]\n",
    "Detection\t99.56\t98.91\t99.71\n",
    "Segmentation\t98.18\t98.43\t98.62\n",
    "D.3Anomaly detection and localization on MVTec AD\n",
    "Based on the previous analysis on the refinement network, we report the performance improvement of refinement network attached to ReConPatch in Table S8. We only compare the average score over the entire class of MVTec AD dataset, rather than score per each class. Table S8 verifies that the refined anomaly score map achieves better performance in both image-level and pixel-level AUROC.\n",
    "\n",
    "Ensemble Backbone\tWRN-101 & RNext-101 & DenseN-201\n",
    "Image size\t480 × 480\t320 × 320\n",
    "Method\tReConPatch\tReConPatch(w/ refine)\tReConPatch\tReConPatch(w/ refine)\n",
    "Detection\t99.72\t99.86\t99.67\t99.78\n",
    "Segmentation\t98.67\t99.20\t98.36\t98.96\n",
    "Table S8:Comparison of ensemble model anomaly detection(image-level AUROC) and segmentation (pixel-level AUROC) performance on the MVTec AD dataset\n",
    "We also visualize the qualitative difference in the anomaly score map in Figure S5. The second row and the third row depict the score map of ReConPatch without and with refinement network respectively. Refined score maps show more fine-grained localization of defects in the images. The performance improvement also rises from reducing the anomaly scores on the irrelevant locations, such as background. The bottom row shows the pixel-wise roc curve for each anomaly score map, which indicates that the refined score map surpasses the score map without refinement.\n",
    "\n",
    "Refer to caption\n",
    "Figure S5:Visualization of the anomaly score map comparing before and after the refinement.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    api_key_filepath = 'gemini_api_key.txt'\n",
    "    model_name = 'models/gemini-1.5-flash-latest'\n",
    "\n",
    "    main(document, api_key_filepath, model_name, chunk_size=3000, overlap_size=100, max_concurrent_requests=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
